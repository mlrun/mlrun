{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Serving ML/DL models\n",
    "\n",
    "This notebook demonstrate how to serve standard ML/DL models using **MLRun Serving**.\n",
    "\n",
    "Make sure you went over the basics in MLRun [**Quick Start Tutorial**](./01-mlrun-basics.html).\n",
    "\n",
    "\n",
    "MLRun serving can produce managed real-time serverless pipelines from various tasks, including MLRun models or standard model files.\n",
    "The pipelines use the Nuclio real-time serverless engine, which can be deployed anywhere.\n",
    "[Nuclio](https://nuclio.io/) is a high-performance open-source \"serverless\" framework that's focused on data, I/O, and compute-intensive workloads.\n",
    "\n",
    "\n",
    "MLRun serving supports advanced real-time data processing and model serving pipelines.<br>\n",
    "For more details and examples, see the {ref}`MLRun serving pipelines <serving>` documentation.\n",
    "\n",
    "Tutorial steps:\n",
    "- [**Using pre-built MLRun serving classes and images**](#pre-built-serving)\n",
    "- [**Create and test the Serving Function**](#create-function)\n",
    "- [**Deploy the serving Function**](#deploy-serving)\n",
    "- [**Build a custom serving class**](#custom-class)\n",
    "- [**Building advanced model Serving Graph**](#serving=graph)\n",
    "\n",
    "## MLRun installation and configuration\n",
    "\n",
    "Before running this notebook make sure the `mlrun` package is installed (`pip install mlrun`) and that you have configured the access to MLRun service. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install MLRun if not installed, run this only once (restart the notebook after the install !!!)\n",
    "# %pip install mlrun"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Get or create a new project:**\n",
    "\n",
    "You should create, load or use (get) an {ref}`MLRun Project<Projects>`. The `get_or_create_project()` method tries to load the project from the MLRun DB. If the project does not exist it creates a new one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> 2022-08-24 08:53:27,460 [info] loaded project tutorial from MLRun DB\n"
     ]
    }
   ],
   "source": [
    "import mlrun\n",
    "project = mlrun.get_or_create_project(\"tutorial\", context=\"src/\", user_project=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"pre-built-serving\"></a>\n",
    "## Using pre-built MLRun serving classes and images\n",
    "\n",
    "MLRun contains built-in serving functionality for the major ML/DL frameworks (Scikit-Learn, TensorFlow.Keras, ONNX, XGBoost, LightGBM and PyTorch). In addition MLRun provide a few container images with the required ML/DL packages pre-installed.\n",
    "\n",
    "You can overwrite the packages in the images, or provide your own image (just need to make sure that the `mlrun` package is installed in it).  \n",
    "\n",
    "The following table specifies, for each framework, the relevant pre-integrated image and the corresponding MLRun `ModelServer` serving class:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|framework       |image          |serving class                               |\n",
    "|:---------------|:--------------|:-------------------------------------------|\n",
    "|SciKit-Learn    |mlrun/mlrun    |mlrun.frameworks.sklearn.SklearnModelServer |\n",
    "|TensorFlow.Keras|mlrun/ml-models|mlrun.frameworks.tf_keras.TFKerasModelServer|\n",
    "|ONNX            |mlrun/ml-models|mlrun.frameworks.onnx.ONNXModelServer       |\n",
    "|XGBoost         |mlrun/ml-models|mlrun.frameworks.xgboost.XGBoostModelServer | \n",
    "|LightGBM        |mlrun/ml-models|mlrun.frameworks.lgbm.LGBMModelServer       |\n",
    "|PyTorch         |mlrun/ml-models|mlrun.frameworks.pytorch.PyTorchModelServer |\n",
    "\n",
    "> For GPU support use the `mlrun/ml-models-gpu` image (adding GPU drivers and support)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example using SKlearn and TF Keras models**\n",
    "\n",
    "See how to specify the parameters in the following two examples. These use standard pre-trained models (using the iris dataset) stored in MLRun samples repository. (You can use your own models instead.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_dir = mlrun.get_sample_path('models/serving/')\n",
    "\n",
    "framework = 'sklearn'  # change to 'keras' to try the 2nd option \n",
    "kwargs = {}\n",
    "if framework == \"sklearn\":\n",
    "    serving_class = 'mlrun.frameworks.sklearn.SklearnModelServer'\n",
    "    model_path = models_dir + 'sklearn.pkl'\n",
    "    image = 'mlrun/mlrun'\n",
    "else:\n",
    "    serving_class = 'mlrun.frameworks.tf_keras.TFKerasModelServer'\n",
    "    model_path = models_dir + 'keras.h5'\n",
    "    image = 'mlrun/ml-models'  # or mlrun/ml-models-gpu when using GPUs\n",
    "    kwargs['labels'] = {'model-format': 'h5'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logging the model\n",
    "\n",
    "The model and its metadata are first registered in MLRun's **Model Registry**. Use the `log_model()` method to specify the model files and metadata (metrics, schema, parameters, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_object = project.log_model(f'{framework}-model', model_file=model_path, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"create-function\"></a>\n",
    "## Create and test the serving function \n",
    "\n",
    "Create a new **`serving`** function, specify its `name` and the correct `image` (with your desired framework).\n",
    "\n",
    "> If you want to add specific packages to the base image, specify the `requirements` attribute, example:\n",
    "> \n",
    "> ```python\n",
    "> serving_fn = mlrun.new_function(\"serving\", image=image, kind=\"serving\", requirements=[\"tensorflow==2.8.1\"])\n",
    "> ```\n",
    "\n",
    "The following example uses a basic topology of a model `router` and adds a single model behind it (you can add multiple models to the same function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.43.0 (0)\n",
       " -->\n",
       "<!-- Title: mlrun&#45;flow Pages: 1 -->\n",
       "<svg width=\"272pt\" height=\"52pt\"\n",
       " viewBox=\"0.00 0.00 272.28 52.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 48)\">\n",
       "<title>mlrun&#45;flow</title>\n",
       "<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-48 268.28,-48 268.28,4 -4,4\"/>\n",
       "<!-- _start -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>_start</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"31.43,-4.05 33.2,-4.15 34.96,-4.3 36.69,-4.49 38.39,-4.74 40.05,-5.03 41.67,-5.36 43.23,-5.75 44.73,-6.18 46.17,-6.65 47.54,-7.16 48.84,-7.71 50.06,-8.31 51.2,-8.94 52.25,-9.61 53.22,-10.31 54.09,-11.04 54.87,-11.8 55.56,-12.59 56.15,-13.41 56.65,-14.25 57.05,-15.11 57.36,-15.99 57.58,-16.89 57.7,-17.8 57.73,-18.72 57.68,-19.65 57.54,-20.59 57.31,-21.53 57.01,-22.47 56.63,-23.41 56.18,-24.35 55.66,-25.28 55.08,-26.2 54.43,-27.11 53.72,-28.01 52.97,-28.89 52.16,-29.75 51.3,-30.59 50.41,-31.41 49.48,-32.2 48.51,-32.96 47.51,-33.69 46.48,-34.39 45.43,-35.06 44.35,-35.69 43.26,-36.29 42.15,-36.84 41.02,-37.35 39.89,-37.82 38.74,-38.25 37.58,-38.64 36.42,-38.97 35.24,-39.26 34.07,-39.51 32.89,-39.7 31.71,-39.85 30.52,-39.95 29.34,-40 28.15,-40 26.96,-39.95 25.78,-39.85 24.6,-39.7 23.42,-39.51 22.24,-39.26 21.07,-38.97 19.91,-38.64 18.75,-38.25 17.6,-37.82 16.46,-37.35 15.34,-36.84 14.23,-36.29 13.13,-35.69 12.06,-35.06 11.01,-34.39 9.98,-33.69 8.98,-32.96 8.01,-32.2 7.08,-31.41 6.18,-30.59 5.33,-29.75 4.52,-28.89 3.76,-28.01 3.06,-27.11 2.41,-26.2 1.83,-25.28 1.3,-24.35 0.85,-23.41 0.47,-22.47 0.17,-21.53 -0.05,-20.59 -0.19,-19.65 -0.25,-18.72 -0.21,-17.8 -0.09,-16.89 0.13,-15.99 0.43,-15.11 0.84,-14.25 1.34,-13.41 1.93,-12.59 2.62,-11.8 3.4,-11.04 4.27,-10.31 5.24,-9.61 6.29,-8.94 7.43,-8.31 8.65,-7.71 9.94,-7.16 11.31,-6.65 12.75,-6.18 14.26,-5.75 15.82,-5.36 17.44,-5.03 19.1,-4.74 20.79,-4.49 22.53,-4.3 24.28,-4.15 26.06,-4.05 27.85,-4 29.64,-4 31.43,-4.05\"/>\n",
       "<text text-anchor=\"middle\" x=\"28.74\" y=\"-18.3\" font-family=\"Times,serif\" font-size=\"14.00\">start</text>\n",
       "</g>\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title></title>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"151.49,-14.54 151.49,-29.46 135.67,-40 113.3,-40 97.49,-29.46 97.49,-14.54 113.3,-4 135.67,-4 151.49,-14.54\"/>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"155.49,-12.4 155.49,-31.6 136.88,-44 112.09,-44 93.49,-31.6 93.49,-12.4 112.09,0 136.88,0 155.49,-12.4\"/>\n",
       "</g>\n",
       "<!-- _start&#45;&gt; -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>_start&#45;&gt;</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M57.36,-22C65.51,-22 74.61,-22 83.38,-22\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"83.45,-25.5 93.45,-22 83.45,-18.5 83.45,-25.5\"/>\n",
       "</g>\n",
       "<!-- sklearn -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>sklearn</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"227.88\" cy=\"-22\" rx=\"36.29\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"227.88\" y=\"-18.3\" font-family=\"Times,serif\" font-size=\"14.00\">sklearn</text>\n",
       "</g>\n",
       "<!-- &#45;&gt;sklearn -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>&#45;&gt;sklearn</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M155.63,-22C163.69,-22 172.58,-22 181.27,-22\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"181.31,-25.5 191.31,-22 181.31,-18.5 181.31,-25.5\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.dot.Digraph at 0x7f05f5467f40>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "serving_fn = mlrun.new_function(\"serving\", image=image, kind=\"serving\", requirements={})\n",
    "serving_fn.add_model(framework ,model_path=model_object.uri, class_name=serving_class, to_list=True)\n",
    "\n",
    "# Plot the serving topology input -> router -> model\n",
    "serving_fn.plot(rankdir=\"LR\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Simulating the model server locally (using the mock_server):**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> 2022-08-24 08:53:27,726 [warning] run command, file or code were not specified\n",
      "> 2022-08-24 08:53:32,250 [info] model sklearn was loaded\n",
      "> 2022-08-24 08:53:32,252 [info] Loaded ['sklearn']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trying to unpickle estimator DecisionTreeClassifier from version 0.23.2 when using version 1.0.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/modules/model_persistence.html#security-maintainability-limitations\n",
      "Trying to unpickle estimator RandomForestClassifier from version 0.23.2 when using version 1.0.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/modules/model_persistence.html#security-maintainability-limitations\n"
     ]
    }
   ],
   "source": [
    "# create a mock server that represents the serving pipeline\n",
    "server = serving_fn.to_mock_server()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "**Test the mock model server endpoint:**\n",
    "    \n",
    "- List the served models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'models': ['sklearn']}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "server.test(\"/v2/models/\", method=\"GET\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Infer using test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '28bab37fdc0a4ec19e446191894f9f6c',\n",
       " 'model_name': 'sklearn',\n",
       " 'outputs': [0, 2]}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = {\"inputs\":[[5.1, 3.5, 1.4, 0.2],[7.7, 3.8, 6.7, 2.2]]}\n",
    "server.test(path=f'/v2/models/{framework}/infer',body=sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "> See more API options and parameters in the **{ref}`Model serving API documentation <model-api>`**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"deploy-serving\"></a>\n",
    "## Deploy the serving function\n",
    "\n",
    "Deploy the serving function and use `invoke` to test it with the provided `sample`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> 2022-08-24 08:53:32,334 [warning] it is recommended to use k8s secret (specify secret_name), specifying the aws_access_key/aws_secret_key directly is unsafe\n",
      "> 2022-08-24 08:53:32,343 [info] Starting remote function deploy\n",
      "2022-08-24 08:53:32  (info) Deploying function\n",
      "2022-08-24 08:53:32  (info) Building\n",
      "2022-08-24 08:53:32  (info) Staging files and preparing base images\n",
      "2022-08-24 08:53:32  (info) Building processor image\n",
      "2022-08-24 08:57:27  (info) Build complete\n",
      "2022-08-24 08:57:48  (info) Function deploy complete\n",
      "> 2022-08-24 08:57:49,042 [info] successfully deployed function: {'internal_invocation_urls': ['nuclio-tutorial-jovyan-serving.mlrun.svc.cluster.local:8080'], 'external_invocation_urls': ['localhost:32129']}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DeployStatus(state=ready, outputs={'endpoint': 'http://localhost:32129', 'name': 'tutorial-jovyan-serving'})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "serving_fn.with_code(body=\" \") # adds the serving wrapper, not required with MLRun >= 1.0.3\n",
    "project.deploy_function(serving_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> 2022-08-24 08:57:49,138 [info] invoking function: {'method': 'POST', 'path': 'http://nuclio-tutorial-jovyan-serving.mlrun.svc.cluster.local:8080/v2/models/sklearn/infer'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'id': '40325ba7-3f1f-461c-9aa1-4fc66b2e196e',\n",
       " 'model_name': 'sklearn',\n",
       " 'outputs': [0, 2]}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "serving_fn.invoke(path=f'/v2/models/{framework}/infer',body=sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"custom-class\"></a>\n",
    "## Building a custom serving class\n",
    "\n",
    "Model serving classes implement the full model serving functionality, which include loading models, pre- and post-processing, prediction, explainability, and model monitoring.\n",
    "\n",
    "Model serving classes must inherit from `mlrun.serving.V2ModelServer`, and at the minimum implement the `load()` (download the model file(s) and load the model into memory) and `predict()` (accept request payload and return prediction/inference results) methods.\n",
    "\n",
    "For more detailed information on custom serving classes, see {ref}`Creating a custom model serving class <custom-model-serving-class>`.\n",
    "\n",
    "The following code demonstrates a minimal scikit-learn (a.k.a. sklearn) serving-class implementation:\n",
    "\n",
    "\n",
    "```python\n",
    "from cloudpickle import load\n",
    "import numpy as np\n",
    "from typing import List\n",
    "import mlrun\n",
    "\n",
    "class ClassifierModel(mlrun.serving.V2ModelServer):\n",
    "    def load(self):\n",
    "        \"\"\"load and initialize the model and/or other elements\"\"\"\n",
    "        model_file, extra_data = self.get_model('.pkl')\n",
    "        self.model = load(open(model_file, 'rb'))\n",
    "\n",
    "    def predict(self, body: dict) -> List:\n",
    "        \"\"\"Generate model predictions from sample.\"\"\"\n",
    "        feats = np.asarray(body['inputs'])\n",
    "        result: np.ndarray = self.model.predict(feats)\n",
    "        return result.tolist()\n",
    "```\n",
    "\n",
    "In order to create a function that incorporates the code of the new class (in `serving.py` ) use `code_to_function`:\n",
    "\n",
    "```python\n",
    "serving_fn = mlrun.code_to_function('serving', filename='serving.py', kind='serving',image='mlrun/mlrun')\n",
    "serving_fn.add_model('my_model',model_path=model_file, class_name='ClassifierModel')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"serving=graph\"></a>\n",
    "## Building an advanced model serving graph\n",
    "\n",
    "MLRun graphs enable building and running DAGs (directed acyclic graph). Graphs are composed of individual steps. \n",
    "The first graph element accepts an `Event` object, transforms/processes the event and passes the result to the next step\n",
    "in the graph, and so on. The final result can be written out to a destination (file, DB, stream, etc.) or returned back to the caller \n",
    "(one of the graph steps can be marked with `.respond()`). \n",
    "\n",
    "The serving graphs can be composed of pre-defined graph steps, block-type elements (model servers, routers, ensembles, \n",
    "data readers and writers, data engineering tasks, validators, etc.), custom steps, or from native python \n",
    "classes/functions. A graph can have data processing steps, model ensembles, model servers, post-processing, etc. \n",
    "Graphs can auto-scale and span multiple function containers (connected through streaming protocols).\n",
    "\n",
    "See the **{ref}`Advanced Model Serving Graph Notebook Example <graph-example>`**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Done!\n",
    "\n",
    "Congratulations! You've completed Part 3 of the MLRun getting-started tutorial.\n",
    "Proceed to [**Part 4: ML Pipeline**](04-pipeline.ipynb) to learn how to create an automated pipeline for your project."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

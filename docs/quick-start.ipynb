{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quick-Start Guide"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**MLRun** is an end-to-end [open-source](https://github.com/mlrun/mlrun) MLOps solution to manage and automate your\n",
    "entire analytics and machine learning lifecycle, from data ingestion, through model development to full pipeline deployment.\n",
    "MLRun is running as a built-in service in Iguazio and is integrated well with other services in the platform.\n",
    "Its primary goal is to ease the development of machine learning pipeline at scale and help organizations build a\n",
    "robust process for moving from the research phase to fully operational production deployments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Table of Contents**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [Working with MLRun](#working-with-mlrun)\n",
    "  - [Train a Model](#train-a-model)\n",
    "  - [Test the Model](#test-the-model)\n",
    "  - [Serve the Model](#serve-the-model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with MLRun\n",
    "<a name=\"working-with-mlrun\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you need to install MLRun, refer to the [Installation Guide](install.md).\n",
    ">**Note**: If you are using the [Iguazio Data Science Platform](https://www.iguazio.com/), MLRun already comes\n",
    ">preinstalled and integrated in your system.\n",
    "\n",
    "If you are not viewing this quick-start guide from a Jupyter Lab instance, open it on your cluster, create a\n",
    "new notebook, and copy the sections below to the notebook to run them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before you begin, initialize MLRun by calling `set_environment` and provide it the project name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlrun import set_environment\n",
    "\n",
    "project_name = 'quick-start'\n",
    "_, artifact_path = set_environment(project=project_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a Model\n",
    "<a name=\"train-a-model\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MLRun introduces the concept of [functions](./runtimes/functions.md). You can run your own code as functions, or use\n",
    "functions from the function marketplace.\n",
    "In the example below, we'll use the [`sklearn_classifier`](https://github.com/mlrun/functions/tree/master/sklearn_classifier)\n",
    "from the function marketplace to train a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlrun import import_function\n",
    "from mlrun.platforms import auto_mount\n",
    "\n",
    "train = import_function('hub://sklearn_classifier').apply(auto_mount())\n",
    "\n",
    "train_run = train.run(name='train',\n",
    "                      inputs={'dataset': 'https://s3.wasabisys.com/iguazio/data/iris/iris_dataset.csv'},\n",
    "                      params={'model_pkg_class': 'sklearn.linear_model.LogisticRegression',\n",
    "                              'label_column': 'label'},\n",
    "                      project=project_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The run output above contains a link to the MLRun UI. Click it to inspect the various aspects of the jobs you run:\n",
    "\n",
    "<img src=\"./_static/images/mlrun-quick-start/train-info.png\" alt=\"ui-info\" width=\"800\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As well as their artifacts:\n",
    "\n",
    "<img src=\"./_static/images/mlrun-quick-start/train-artifacts.png\" alt=\"ui-artifacts\" width=\"800\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When running the function in a Jupyter notebook, the output cell for your function execution will contain a table with\n",
    "run information &mdash; including the state of the execution, all inputs and parameters, and the execution results and artifacts.\n",
    "\n",
    "![MLRun quick start train output](./_static/images/mlrun-quick-start/train-output.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the Model\n",
    "<a name=\"test-the-model\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have a trained model, you can test it: run a task that uses the [test_classifier](https://github.com/mlrun/functions/tree/master/test_classifier)\n",
    "function from the function marketplace to run the selected trained model against the test dataset. The test dataset\n",
    "was returned from the training task (`train_run`) in the previous step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = import_function('hub://test_classifier').apply(auto_mount())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can then run the function as part of your project, just as any other function that you have written yourself.\n",
    "To view the function documentation, call the `doc` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "function: test-classifier\n",
      "test a classifier using held-out or new data\n",
      "default handler: test_classifier\n",
      "entry points:\n",
      "  test_classifier: Test one or more classifier models against held-out dataset\n",
      "\n",
      "Using held-out test features, evaluates the peformance of the estimated model\n",
      "\n",
      "Can be part of a kubeflow pipeline as a test step that is run post EDA and \n",
      "training/validation cycles\n",
      "    context  - the function context, default=\n",
      "    models_path(DataItem)  - artifact models representing a file or a folder, default=\n",
      "    test_set(DataItem)  - test features and labels, default=\n",
      "    label_column(str)  - column name for ground truth labels, default=\n",
      "    score_method(str)  - for multiclass classification, default=micro\n",
      "    plots_dest(str)  - dir for test plots, default=\n",
      "    model_evaluator  - NOT IMPLEMENTED: specific method to generate eval, passed in as string or available in this folder, default=None\n",
      "    default_model(str)  - , default=model.pkl\n",
      "    predictions_column(str)  - column name for the predictions column on the resulted artifact, default=yscore\n",
      "    model_update  - (True) update model, when running as stand alone no need in update, default=True\n"
     ]
    }
   ],
   "source": [
    "test.doc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configure parameters for the test function (`params`), and provide the selected trained model from the train task as an input artifact (`inputs`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_run = test.run(name=\"test\",\n",
    "                    params={\"label_column\": \"label\"},\n",
    "                    inputs={\"models_path\": train_run.outputs['model'],\n",
    "                            \"test_set\": train_run.outputs['test_set']},\n",
    "                    project=project_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Serve the Model\n",
    "<a name=\"serve-the-model\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MLRun serving can take MLRun models or standard model files and produce managed, real-time, serverless functions using\n",
    "the [Nuclio real-time serverless framework](https://www.iguazio.com/open-source/nuclio/).\n",
    "Nuclio is built around data, I/O, and compute intensive workloads and is focused on performance and flexibility.\n",
    "Nuclio is also deeply integrated into the MLRun framework.\n",
    "See [MLRun Serving documentation](./serving/serving-graph.md) to learn more about the rich serving capabilities\n",
    "MLRun has to offer.\n",
    "\n",
    "\n",
    "To deploy your model using the [v2_model_server function](https://github.com/mlrun/functions/tree/master/v2_model_server),\n",
    "run the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "serve = import_function('hub://v2_model_server').apply(auto_mount())\n",
    "model_name='iris'\n",
    "serve.add_model(model_name, model_path=train_run.outputs['model'])\n",
    "addr = serve.deploy(project=project_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `invoke` method enables to programmatically test the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "inputs = [[5.1, 3.5, 1.4, 0.2],\n",
    "          [7.7, 3.8, 6.7, 2.2]]\n",
    "my_data = json.dumps({'inputs': inputs})\n",
    "serve.invoke(f'v2/models/{model_name}/infer', my_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Open the Nuclio UI to view the function and test it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Nuclio Functions UI](./_static/images/mlrun-quick-start/serve.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a more detailed walk-through, refer to the [End-to-end Pipeline Tutorial](end-to-end-pipeline.rst)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "(monitoring-models)=\n",
    "# View model monitoring results in the platform UI and in Grafana\n",
    "\n",
    "**In this section**\n",
    "- [Model monitoring in the platform UI](#model-monitoring-in-the-platform-ui)\n",
    "- [Model monitoring in the Grafana dashboards](#model-monitoring-in-the-grafana-dashboards)\n",
    "\n",
    "\n",
    "## Model monitoring in the platform UI\n",
    "Iguazio's model monitoring data is available for viewing through the regular platform UI.\n",
    "The platform provides these information pages with model monitoring data.\n",
    "\n",
    "* [Model endpoint summary list](#model-endpoint-summary-list)\n",
    "* [Model endpoint overview](#model-endpoint-overview)\n",
    "* [Model features analysis](#model-features-analysis)\n",
    "\n",
    "\n",
    "### Model endpoint summary list\n",
    "\n",
    "1. Select a project from the project tiles screen.\n",
    "2. From the project dashboard, press the **Models** tile to view the models currently deployed.\n",
    "2. Press **Model Endpoints** from the menu to display a list of monitored endpoints.<br>\n",
    "   If the Model Monitoring feature is not enabled, the endpoints list is empty.\n",
    "   \n",
    "   \n",
    "The Model Endpoints summary list provides a quick view of the model monitoring data.\n",
    "\n",
    "![Model Monitoring Summary List](../_static/images/model_endpoints_main.png)\n",
    "\n",
    "The summary page contains the following fields:\n",
    "* **Name** &mdash; the name of the model endpoint\n",
    "* **Function** &mdash; the name of the related function  \n",
    "* **Version** &mdash; user configured version taken from model deployment\n",
    "* **Class** &mdash; the implementation class that is used by the endpoint\n",
    "* **Labels** &mdash; user configurable tags that are searchable\n",
    "* **Uptime** &mdash; first request for production data\n",
    "* **Last Prediction** &mdash; most recent request for production data\n",
    "* **Average Latency** &mdash; Average latency time of serving a single event in the last hour\n",
    "* **Error Count** &mdash; includes prediction process errors such as operational issues (For example, a function in a failed state), as well as data processing errors\n",
    "(For example, invalid timestamps, request ids, type mismatches etc.)\n",
    "* **Drift** &mdash; indication of drift status (no drift (green), possible drift (yellow), drift detected (red))\n",
    "\n",
    "### Model endpoint overview\n",
    "The Model Endpoints overview pane displays general information about the selected model.\n",
    "\n",
    "![Model Endpoints Overview](../_static/images/IG_model_endpoints_overview.png)\n",
    "\n",
    "The Overview page contains the following fields:\n",
    "* **UUID** &mdash; the ID of the deployed model\n",
    "* **Model Class** &mdash; the implementation class that is used by the endpoint\n",
    "* **Model Artifact** &mdash; reference to the model's file location\n",
    "* **Function URI** &mdash; the MLRun function to access the model\n",
    "* **Function Tag** &mdash; the MLRun function tag\n",
    "* **Feature set** &mdash; the monitoring feature set that points to the monitoring parquet directory\n",
    "* **Last Prediction** &mdash; most recent request for production data\n",
    "* **Error Count** &mdash; includes prediction process errors such as operational issues (For example, a function in a failed state), as well as data processing errors\n",
    "(For example, invalid timestamps, request ids, type mismatches etc.)\n",
    "* **Accuracy** &mdash; a numeric value representing the accuracy of model predictions (N/A)\n",
    "* **Stream path** &mdash; the input and output stream of the selected model\n",
    "* **Mean TVD** &mdash; the mean value of the [Total Variance Distance](../monitoring/model-monitoring-deployment.html#:~:text=Total%20Variation%20Distance%20(TVD)%20%E2%80%94%20The%20statistical%20difference%20between%20the%20actual%20predictions%20and%20the%20model%E2%80%99s%20trained%20predictions) of the model features and labels\n",
    "* **Mean Hellinger** &mdash; the mean value of the [Hellinger Distance](../monitoring/model-monitoring-deployment.html#:~:text=Hellinger%20Distance%20%E2%80%94%20A%20type%20of%20f%2Ddivergence%20that%20quantifies%20the%20similarity%20between%20the%20actual%20predictions%2C%20and%20the%20model%E2%80%99s%20trained%20predictions) of the model features and labels\n",
    "* **Mean KLD** &mdash; the mean value of the [KL Divergence](../monitoring/model-monitoring-deployment.html#:~:text=Kullback%E2%80%93Leibler%20Divergence%20(KLD)%20%E2%80%94%20The%20measure%20of%20how%20the%20probability%20distribution%20of%20actual%20predictions%20is%20different%20from%20the%20second%20model%E2%80%99s%20trained%20reference%20probability%20distribution) of the model features and labels\n",
    "* **Drift Actual Value** &mdash; the resulted drift value of the latest drift analysis calculation. \n",
    "* **Drift Detected Threshold** &mdash; pre-defined value to determine a drift \n",
    "* **Possible Drift Threshold** &mdash; pre-defined value to determine a possible drift\n",
    "\n",
    "```{note}\n",
    "Press **Resource monitoring** to get the relevant [Grafana Model Monitoring Details Dashboard](#model-monitoring-details-dashboard) that displays detailed, real-time performance data of the selected model. In addition, use the ellipsis to view the YAML resource file for details about the monitored resource.\n",
    "```\n",
    "\n",
    "### Model features analysis\n",
    "The Features Analysis pane provides details of the drift analysis in a table format with each feature and label in the selected model on its own line.\n",
    "\n",
    "![Model Endpoints Features Analysis](../_static/images/IG_model_endpoints_features_analysis.png)\n",
    "\n",
    "Each field has a pair of columns. The **Expected** column displays the results from the model training phase, and the **Actual** column\n",
    "displays the results from the live production data. The following fields are available:\n",
    "* **Mean**\n",
    "* **STD** (Standard deviation)\n",
    "* **Min**\n",
    "* **Max**\n",
    "* **TVD**\n",
    "* **Hellinger**\n",
    "* **KLD**\n",
    "* **Histograms**&mdash;the approximate representation of the distribution of the data. Hover over the bars in the graph for  the details.\n",
    "\n",
    "## Model monitoring in the Grafana dashboards\n",
    "You can deploy a Grafana service in your Iguazio instance and use Grafana Dashboards to view model monitoring details.\n",
    "There are four dashboards:\n",
    "* [Overview dashboard](#model-endpoints-overview-dashboard)\n",
    "* [Details dashboard](#model-monitoring-details-dashboard)\n",
    "* [Performance dashboard](#model-monitoring-performance-dashboard)\n",
    "* [Applications dashboard](#model-monitoring-applications-dashboard)\n",
    "\n",
    "```{note}\n",
    "You need to train and deploy a model to see results in the dashboards.\n",
    "The dashboards immediately display data if you already have a model that is trained and running with production data.\n",
    "```\n",
    "\n",
    "### Model monitoring Overview dashboard\n",
    "The Overview dashboard displays the model endpoint IDs of a specific project. Only deployed models with Model Monitoring enabled are displayed.\n",
    "Endpoint IDs are URIs used to provide access to performance data and drift detection statistics of a deployed model.\n",
    "\n",
    "![overview](../_static/images/overview.png)\n",
    "\n",
    "The Overview pane provides details about the performance of all the deployed and monitored models within a project. You can change projects by choosing a new project from the\n",
    "**Project** dropdown. The Overview dashboard displays the number of endpoints in the project, the average predictions per second (using a 5-minute rolling average),\n",
    "the average latency (using a 1-hour rolling average), and the total error count in the project.\n",
    "\n",
    "Additional details include:\n",
    "* **Endpoint ID** &mdash; the ID of the deployed model. Use this link to drill down to the model performance and details panes.\n",
    "* **Function** &mdash; the MLRun function to access the model\n",
    "* **Model** &mdash; user defined name for the model\n",
    "* **Model Class** &mdash; the implementation class that is used by the endpoint\n",
    "* **First Request** &mdash; first request for production data\n",
    "* **Last Request** &mdash; most recent request for production data\n",
    "* **Error Count** &mdash; includes prediction process errors such as operational issues (for example, a function in a failed state), as well as data processing errors (for example, invalid timestamps, request ids, type mismatches etc.)\n",
    "* **Drift Status** &mdash; no drift (green), possible drift (yellow), drift detected (red)\n",
    "\n",
    "At the bottom of the dashboard are heat maps for the Predictions per second (5 minute average), Average Latency (one hour average), and Errors.\n",
    "See [How to Read a Heat Map](#how-to-read-a-heat-map) for more details.\n",
    "\n",
    "Click an endpoint ID to drill down to the performance details of that model.\n",
    "\n",
    "#### How to read a heat map\n",
    "Heat maps are used to analyze trends and to instantly transform and enhance data through visualizations. This helps to quickly identify areas of interest,\n",
    "and empower users to explore the data in order to pinpoint where there may be potential issues. A heat map uses a matrix layout with colour and shading to show the relationship between\n",
    "two categories of values (x and y axes), so the darker the cell, the higher the value. The values presented along each axis correspond to a cell which is color-coded to represent the relationship between\n",
    "the two categories. The Predictions per second heatmap shows the relationship between time, and the predictions per second, and the Average Latency per hour shows the relationship between\n",
    "time and the latency.\n",
    "\n",
    "To properly read the heap maps, follow the hierarchy of shades from the darkest (the highest values) to the lightest shades (the lowest values).\n",
    "\n",
    "```{note}\n",
    "The exact quantitative values represented by the colors may be difficult to determine. Use the [Performance Dashboard](#model-endpoint-performance-dashboard) to see detailed results.\n",
    "```\n",
    "\n",
    "### Model monitoring Details dashboard\n",
    "The model monitoring details dashboard displays the real-time performance data of the selected model in detail.\n",
    "Model performance data provided is rich and is used to fine tune or diagnose potential performance issues that may affect business goals.\n",
    "The data in this dashboard changes based on the selection of the project and model.\n",
    "\n",
    "This dashboard panes are:\n",
    "\n",
    "- [Project and model summary](#project-and-model-summary)\n",
    "- [Overall drift analysis](#overall-drift-analysis)\n",
    "- [Incoming features graph](#incoming-features-graph)\n",
    "\n",
    "![details](../_static/images/details.png)\n",
    "\n",
    "#### Project and model summary\n",
    "Use the dropdown to change the project and model. Results are based on the last run only. The dashboard presents the following information about the project:\n",
    "* **Endpoint ID** &mdash; the ID of the deployed model\n",
    "* **Model** &mdash; user defined name for the model\n",
    "* **Function URI** &mdash; the MLRun function to access the model\n",
    "* **Model Class** &mdash; the implementation class that is used by the endpoint\n",
    "* **Prediction/s** &mdash; the average number of predictions per second over a rolling 5-minute period\n",
    "* **Average Latency** &mdash; the average latency over a rolling 1-hour period\n",
    "* **First Request** &mdash; first request for production data\n",
    "* **Last Request** &mdash; most recent request for production data\n",
    "\n",
    "\n",
    "#### Overall Drift Analysis\n",
    "This pane has two sections: Overall Drift Analysis and Features Analysis. Results are based on the last run only. \n",
    "The Overall Drift Analysis pane provides performance statistics for the currently selected model.\n",
    "* **TVD** (sum and mean)\n",
    "* **Hellinger** (sum and mean)\n",
    "* **KLD** (sum and mean)\n",
    "\n",
    "#### Incoming features graph\n",
    "This graph displays the performance of the features that are in the selected model based on sampled data points from actual feature production data.\n",
    "The graph displays the values of the features in the model over time.\n",
    "\n",
    "### Model monitoring Performance dashboard\n",
    "Model endpoint performance displays performance details in graphical format.\n",
    "\n",
    "![performance](../_static/images/performance.png)\n",
    "\n",
    "This dashboard has five graphs:\n",
    "* **Drift Measures** &mdash; the overall drift over time for each of the endpoints in the selected model\n",
    "* **Average Latency** &mdash; the average latency of the model in 5 minute intervals, for 5 minutes and 1 hour rolling windows\n",
    "* **Predictions/s**  &mdash; the model predictions per second displayed in 5 second intervals for 5 minutes (rolling)\n",
    "* **Predictions Count** &mdash; the number of predictions the model makes for 5 minutes and 1 hour rolling windows\n",
    "* **Custom Metrics** &mdash; user-defined custom metrics\n",
    "\n",
    "### Model monitoring Applications dashboard\n",
    "Model monitoring of deployed applications displays drift details in graphical format.\n",
    "\n",
    "* **Draft Status by Category** &mdash; the percentages of potential drift, no drift and detected drift\n",
    "* **Average Drift Value Result** &mdash; the average of the potential and the detected drift values\n",
    "* **Latest Result** &mdash; summary of the latest draft result \n",
    "* **Application Summary** &mdash; table summary of the application results including the schedule time, the metric \n",
    "name, metric kind, result status, and result numerical value. \n",
    "* **Result Value by Time** &mdash; user-defined metric by time. This value is calculated based on user defined function \n",
    "and represents a specific numerical result of the selected application by time.  \n",
    "* **Drift Detection History** &mdash; drift status by time. Using this chart you can detect different types of drifts by time such as gradual drift or incremental drift. To learn more about different types of drift, see [Concept Drift](https://www.iguazio.com/glossary/concept-drift/). Results that\n",
    "do not represent drift (\"irrelevant\") are filtered from this chart. \n",
    "\n",
    "![applications](../_static/images/mm-applications.png)\n",
    "\n",
    "### Configuring Grafana datasources\n",
    "Verify that you have a Grafana service running in your Iguazio AI Platform.\n",
    "If you do not have a Grafana service running, please follow <a href=\"https://www.iguazio.com/docs/latest-release/services/fundamentals/#create-new-service\" target=\"_blank\">Creating a Service</a> to add it to your platform.\n",
    " When you create the service: In the **Custom Parameters** tab, **Platform data-access user** parameter, select a user with access to the `/user/pipelines` directory.\n",
    "\n",
    "In addition, you will have to add access keys to your model-monitoring data source:\n",
    "   1. Open your Grafana service.\n",
    "   2. Navigate to **Configuration | Data Sources**.\n",
    "   3. Press **model-monitoring**.\n",
    "   4. In Custom HTTP Headers, configure the cookie parameter. Set the value of `cookie` to:\n",
    "   `session=j:{\"sid\": \"<YOUR ACCESS KEY>\"}`\n",
    "   5. Press **Save & Test** for verification. You'll receive a confirmation with either a success or a failure message.\n",
    "\n",
    "<img src=\"../_static/images/model-monitoring-datasource.png\" alt=\"Grafana Model Monitoring Datasource\" width=\"400\"/><br>\n",
    "\n",
    "\n",
    "### Configuring Grafana dashboards\n",
    "From Iguazio 3.5.3, the overview, details, and performance dashboards can be found under **Dashboards | Manage | private**.\n",
    "You can also import the latest dashboards versions by downloading them using the following links:\n",
    "\n",
    "**Iguazio 3.5.3 and higher**\n",
    " * {download}`Model Monitoring - Overview <./dashboards/model-monitoring-overview.json>` \n",
    " * {download}`Model Monitoring - Details <./dashboards/model-monitoring-details.json>`\n",
    " * {download}`Model Monitoring - Performance <./dashboards/model-monitoring-performance.json>`\n",
    "\n",
    "**Iguazio up to and including 3.5.2**\n",
    " * {download}`Model Monitoring - Overview <./dashboards/iguazio-3.5.2-and-older/model-monitoring-overview.json>`\n",
    " * {download}`Model Monitoring - Details <./dashboards/iguazio-3.5.2-and-older/model-monitoring-details.json>`\n",
    " * {download}`Model Monitoring - Performance <./dashboards/iguazio-3.5.2-and-older/model-monitoring-performance.json>`\n",
    "\n",
    "Upload dashboards to your Grafana service by:\n",
    "   1. Navigate to your Grafana service in the Services list and press it.\n",
    "   2. Press the dashboards icon in left menu.\n",
    "   3. In the Dashboard Management screen, press **IMPORT**, and select one file to import. Repeat this step for each dashboard."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

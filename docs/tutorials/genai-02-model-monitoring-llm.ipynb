{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bafef919",
   "metadata": {},
   "source": [
    "(genai-02-mm-llm)=\n",
    "# Model monitoring using LLM\n",
    "\n",
    "\n",
    "## Prerequisites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "33d4d744-4452-465f-96d0-36aa71ceb463",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlrun\n",
    "from mlrun.features import Feature"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01c54e77",
   "metadata": {},
   "source": [
    "Create the project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4825fbe2-89d8-4687-978e-0f4d76684353",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> 2024-10-22 11:06:22,479 [info] Created and saved project: {\"context\":\"./\",\"from_template\":null,\"name\":\"llm-monitoring-intro\",\"overwrite\":false,\"save\":true}\n",
      "> 2024-10-22 11:06:22,481 [info] Project created successfully: {\"project_name\":\"llm-monitoring-intro\",\"stored_in_db\":true}\n"
     ]
    }
   ],
   "source": [
    "project = mlrun.get_or_create_project(name=\"llm-monitoring-intro\", context=\"./\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39914102",
   "metadata": {},
   "source": [
    "Set the credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "690aa9d5-9dd4-4dae-8a37-1d4ba305db63",
   "metadata": {},
   "outputs": [],
   "source": [
    "project.set_model_monitoring_credentials(\n",
    "    os.environ[\"V3IO_ACCESS_KEY\"],\n",
    "    \"v3io\",\n",
    "    \"v3io\",\n",
    "    \"v3io\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f2c7533",
   "metadata": {},
   "source": [
    "Enable model monitoring for the project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16cc6980-47d9-487a-8a98-cb9e3282c23c",
   "metadata": {},
   "outputs": [],
   "source": [
    "project.enable_model_monitoring(\n",
    "    image=\"mlrun/mlrun\",\n",
    "    base_period=2,  # frequency (in minutes) at which the monitoring applications are triggered\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de62f740",
   "metadata": {},
   "source": [
    "## Monitoring\n",
    "\n",
    "The monitoring function code collects the traffic to the serving function, analyzes it, and generates results for the specified metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96df62bc-555c-4700-b118-65b0ccacb913",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile monit-code.py\n",
    "import enum\n",
    "import re\n",
    "from typing import Any, Union\n",
    "\n",
    "import mlrun\n",
    "import mlrun.common.schemas\n",
    "from mlrun.model import ModelObj\n",
    "from mlrun.model_monitoring.applications import (\n",
    "    ModelMonitoringApplicationBase,\n",
    "    ModelMonitoringApplicationResult,\n",
    ")\n",
    "\n",
    "STATUS_RESULT_MAPPING = {\n",
    "    0: mlrun.common.schemas.model_monitoring.constants.ResultStatusApp.detected,\n",
    "    1: mlrun.common.schemas.model_monitoring.constants.ResultStatusApp.no_detection,\n",
    "}\n",
    "\n",
    "\n",
    "class LLMAsAJudgeApplication(ModelMonitoringApplicationBase):\n",
    "\n",
    "    def do_tracking(\n",
    "        self,\n",
    "        monitoring_context,\n",
    "    ) -> Union[\n",
    "        ModelMonitoringApplicationResult, list[ModelMonitoringApplicationResult]\n",
    "    ]:\n",
    "        \n",
    "        # User monitoring sampling, in this case an integer representing model performance\n",
    "        # Can be calulated based off the traffic to the function using monitoring_context.sample_df\n",
    "        result = 0.9\n",
    "\n",
    "        tag = re.sub(pattern, \"-\", str(monitoring_context.end_infer_time))\n",
    "        monitoring_context.log_dataset(\n",
    "            key=\"llm-monitoring-df\",\n",
    "            df=monitoring_context.sample_df\n",
    "        )\n",
    "\n",
    "        # get status:\n",
    "        status = STATUS_RESULT_MAPPING[round(result)]\n",
    "\n",
    "        return ModelMonitoringApplicationResult(\n",
    "            name=\"llm-monitoring-df\",\n",
    "            value=result,\n",
    "            kind=mlrun.common.schemas.model_monitoring.constants.ResultKindApp.model_performance,\n",
    "            status=status,\n",
    "            extra_data={},\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20d1ac66",
   "metadata": {},
   "source": [
    "Define the model monitoring custom function that scans the traffic and calculates the performance metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a912e3b0-0818-451f-8c52-7aa6a2a101d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "application = project.set_model_monitoring_function(\n",
    "    func=\"monit-code.py\",\n",
    "    application_class=\"LLMMonitApplication\",\n",
    "    name=\"llm-monit\",\n",
    "    image=\"mlrun/mlrun\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c627200-07a2-4846-a67c-264bfd3670c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "project.deploy_function(application)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ed1506f",
   "metadata": {},
   "source": [
    "Create a model serving class that loads the LLM and generates responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ed8f7f4-c64b-4065-96ef-697f0c346675",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile model-serving.py\n",
    "import mlrun\n",
    "from mlrun.serving.v2_serving import V2ModelServer\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "\n",
    "class LLMModelServer(V2ModelServer):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        context: mlrun.MLClientCtx = None,\n",
    "        name: str = None,\n",
    "        model_path: str = None,\n",
    "        model_name: str = None,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__(name=name, context=context, model_path=model_path, **kwargs)\n",
    "        self.model_name = model_name\n",
    "    \n",
    "    def load(\n",
    "        self,\n",
    "    ):\n",
    "        # Load the model from Hugging Face\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(self.model_name)\n",
    "\n",
    "\n",
    "    def predict(self, request: dict[str, Any]):\n",
    "        inputs = request.get(\"inputs\", [])\n",
    "      \n",
    "        input_ids, attention_mask = self.tokenizer(\n",
    "            inputs[0], return_tensors=\"pt\"\n",
    "        ).values()\n",
    "\n",
    "        outputs = self.model.generate(input_ids=input_ids, attention_mask=attention_mask)\n",
    "\n",
    "        # Remove input:\n",
    "        outputs = self.tokenizer.decode(outputs[0])\n",
    "        outputs = outputs.split(inputs[0])[-1].replace(self.tokenizer.eos_token, \"\")\n",
    "        return [{\"generated_text\": outputs}]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ad3ab8",
   "metadata": {},
   "source": [
    "Create the serving function using the class you just defined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a94ab4eb-524f-4709-9354-53d8cb67cb98",
   "metadata": {},
   "outputs": [],
   "source": [
    "serving_fn = project.set_function(func=\"model-serving.py\",\n",
    "                     name=\"llm-server\",\n",
    "                     kind=\"serving\",\n",
    "                     image=\"gcr.io/iguazio/llm-serving:1.7.0\")\n",
    "\n",
    "serving_fn.apply(mlrun.auto_mount())\n",
    "\n",
    "# GPU is optional\n",
    "serving_fn.with_limits(gpus=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8bcec42",
   "metadata": {},
   "source": [
    "## Deploy the model, enable tracking, and deploy the function\n",
    "\n",
    "This tutorial uses the gemma-2b model by Google. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f048d80",
   "metadata": {},
   "source": [
    "Log the model to the project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a983b01-3be4-4195-852a-a950fee0d5c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = \"google-gemma-2b\"\n",
    "project.log_model(\n",
    "    base_model,\n",
    "    model_file=\"model-iris.pkl\",\n",
    "    inputs=[Feature(value_type=\"str\", name=\"question\")],\n",
    "    outputs=[Feature(value_type=\"str\", name=\"answer\")],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00bc60ae",
   "metadata": {},
   "source": [
    "Enable tracking for the function, then deploy it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58575f9f-4d41-4408-95af-9db52dd3bd29",
   "metadata": {},
   "outputs": [],
   "source": [
    "serving_function.set_tracking()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95259455-0043-4576-b361-a91cda808df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "deployment = serving_function.deploy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df7bff62",
   "metadata": {},
   "source": [
    "Now the traffic to the function is analyzed and the performance is calculated."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

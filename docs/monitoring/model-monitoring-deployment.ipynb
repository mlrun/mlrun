{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "(legacy-model-monitoring)=\n",
    "# Legacy model monitoring \n",
    "\n",
    "```{note}\n",
    "This legacy mode of model monitoring is currently supported only for the CE version of MLRun.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**In this section**\n",
    "- [Architecture](#architecture)\n",
    "- [Enabling model monitoring](#enabling-model-monitoring)\n",
    "\n",
    "**See also**\n",
    "- [Legacy model monitoring and drift detection tutorial](./legacy-model-monitoring.html)\n",
    "\n",
    "## Architecture\n",
    "The model monitoring process flow starts with collecting operational data. The operational data are converted to vectors, which are posted to the Model Server.\n",
    "The model server is then wrapped around a machine learning model that uses a function to calculate predictions based on the available vectors.\n",
    "Next, the model server creates a log for the input and output of the vectors, and the entries are written to the production data stream (a [v3io stream](https://nuclio.io/docs/latest/reference/triggers/v3iostream/)).\n",
    "While the model server is processing the vectors, a Nuclio operation monitors the log of the data stream and is triggered when a new log entry is detected.\n",
    "The Nuclio function examines the log entry, processes it into statistics which are then written to the statistics databases (parquet file, time series database and key value database).\n",
    "The parquet files are written as a feature set under the model monitoring project. The parquet files can be read either using `pandas.read_parquet` or `feature_set.get_offline_features`, like any other feature set.\n",
    "In parallel, a scheduled MLRun job runs reading the parquet files, performing drift analysis. The drift analysis data is stored so that the user can retrieve it in the Iguazio UI or in a Grafana dashboard.\n",
    "\n",
    "Monitoring is supported by Iguazio's streaming technology, and open-source integration with Kafka.\n",
    "\n",
    "\n",
    "![Architecture](../_static/images/model-monitoring-data-flow.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Enabling model monitoring\n",
    "\n",
    "To see tracking results, model monitoring needs to be enabled in each model.\n",
    "\n",
    "To utilize drift measurement, supply the train set in the training step.\n",
    "\n",
    "Model activities can be tracked into a real-time stream and time-series DB. The monitoring data\n",
    "is used to create real-time dashboards, detect drift, and analyze performance. \n",
    "\n",
    "To monitor a deployed model, apply {py:meth}`~mlrun.runtimes.ServingRuntime.set_tracking` on your serving function and specify the function spec attributes:\n",
    "\n",
    "    \n",
    "   `fn.set_tracking(stream_path, batch, sample, tracking_policy)`\n",
    "    \n",
    "- **stream_path**\n",
    "  - Enterprise: the v3io stream path (e.g. `v3io:///users/..`)\n",
    "  - CE: a valid Kafka stream (e.g. `kafka://kafka.default.svc.cluster.local:9092`)\n",
    "- **sample** &mdash; optional, sample every N requests\n",
    "- **batch** &mdash; optional, send micro-batches every N requests\n",
    "- **tracking_policy** &mdash; optional, model tracking configurations, such as setting the scheduling policy of the model monitoring batch job\n",
    "\n",
    "If a serving function is configured for model-monitoring tracking, use this procedure to change the parameters of the tracking (for example changing the `default_batch_intervals` of the `tracking_policy`):\n",
    "1. Delete the \"model-monitoring-batch\" job function (can be found under ML functions).\n",
    "2. Delete the \"model-monitoring-batch\" schedule job (can be found under Jobs and Workflows -> Schedule).\n",
    "3. Redeploy the serving function with new model-monitoring tracking parameters.\n",
    "    \n",
    "## Model monitoring demo\n",
    "Use the following code to test and explore model monitoring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Set project name\n",
    "project_name = \"demo-project\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploy model servers\n",
    "Use the following code to deploy a model server in the Iguazio instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris\n",
    "import sys\n",
    "\n",
    "import mlrun\n",
    "from mlrun import import_function, get_dataitem, get_or_create_project\n",
    "from mlrun.platforms import auto_mount\n",
    "\n",
    "project = get_or_create_project(project_name, context=\"./\")\n",
    "project.set_model_monitoring_credentials(os.environ.get(\"V3IO_ACCESS_KEY\"))\n",
    "\n",
    "# Download the pre-trained Iris model\n",
    "# We choose the correct model to avoid pickle warnings\n",
    "suffix = (\n",
    "    mlrun.__version__.split(\"-\")[0].replace(\".\", \"_\")\n",
    "    if sys.version_info[1] > 9\n",
    "    else \"3.9\"\n",
    ")\n",
    "model_path = mlrun.get_sample_path(f\"models/model-monitoring/model-{suffix}.pkl\")\n",
    "\n",
    "get_dataitem(model_path).download(\"model.pkl\")\n",
    "\n",
    "iris = load_iris()\n",
    "train_set = pd.DataFrame(\n",
    "    iris[\"data\"],\n",
    "    columns=[\"sepal_length_cm\", \"sepal_width_cm\", \"petal_length_cm\", \"petal_width_cm\"],\n",
    ")\n",
    "\n",
    "# Import the serving function from the Function Hub\n",
    "serving_fn = import_function(\"hub://v2_model_server\", project=project_name).apply(\n",
    "    auto_mount()\n",
    ")\n",
    "\n",
    "model_name = \"RandomForestClassifier\"\n",
    "\n",
    "# Log the model through the projects API so that it is available through the feature store API\n",
    "project.log_model(model_name, model_file=\"model.pkl\", training_set=train_set)\n",
    "\n",
    "# Add the model to the serving function's routing spec\n",
    "serving_fn.add_model(\n",
    "    model_name, model_path=f\"store://models/{project_name}/{model_name}:latest\"\n",
    ")\n",
    "\n",
    "# Enable model monitoring\n",
    "serving_fn.set_tracking()\n",
    "\n",
    "# Deploy the function\n",
    "serving_fn.deploy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulating requests\n",
    "Use the following code to simulate production data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from time import sleep\n",
    "from random import choice, uniform\n",
    "\n",
    "iris_data = iris[\"data\"].tolist()\n",
    "\n",
    "while True:\n",
    "    data_point = choice(iris_data)\n",
    "    serving_fn.invoke(\n",
    "        f\"v2/models/{model_name}/infer\", json.dumps({\"inputs\": [data_point]})\n",
    "    )\n",
    "    sleep(uniform(0.2, 1.7))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

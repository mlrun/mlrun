{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(databricks)=\n",
    "# Databricks runtime\n",
    "\n",
    "The databricks runtime runs on a Databricks cluster (and not in the Iguazio cluster). The function raises a pod on MLRun, which communicates with the Databricks cluster. The requests originate in MLRun and all computing is in the Databricks cluster.\n",
    "\n",
    "With the databricks runtime, you send keyword arguments (kwargs) to this job, send your local file/code as a string to the job, and use a handler as an endpoint for user code.\n",
    "\n",
    "You can run the function on:\n",
    "- An existing cluster, by including `DATABRICKS_CLUSTER_ID`\n",
    "- A job compute cluster, created and dedicated for this function only. Omit `DATABRICKS_CLUSTER_ID` to create a job compute cluster, and set the [cluster specs](https://docs.databricks.com/en/workflows/jobs/jobs-2.0-api.html#newcluster) by using the task parameters, when running the function, For example:\n",
    "   ```\n",
    "   params['task_parameters'] = {'new_cluster_spec': {'node_type_id': 'm5d.xlarge', 'num_workers': 2, 'timeout_minutes': 15}}\n",
    "   ```\n",
    "Do not send variables named `mlrun_internal_code` or `context` since these are utilized by the internal processes of the runtime.\n",
    "\n",
    "Example of running a databricks job from a local file on the existing cluster: DATABRICKS_CLUSTER_ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import mlrun\n",
    "from mlrun.runtimes.function_reference import FunctionReference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If using a Databricks data store, for example, set the credentials:\n",
    "os.environ[\"DATABRICKS_HOST\"] = \"DATABRICKS_HOST-NAME\"\n",
    "os.environ[\"DATABRICKS_TOKEN\"] = \"DATABRICKS_TOKEN\"\n",
    "os.environ[\"DATABRICKS_CLUSTER_ID\"] = \"DATABRICKS_CLUSTER_ID\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project = mlrun.get_or_create_project(\"proj-name\", context=\"./\", user_project=False)\n",
    "\n",
    "job_env = {\n",
    "    \"DATABRICKS_HOST\": os.environ[\"DATABRICKS_HOST\"],\n",
    "    \"DATABRICKS_CLUSTER_ID\": os.environ.get(\"DATABRICKS_CLUSTER_ID\"),\n",
    "}\n",
    "secrets = {\"DATABRICKS_TOKEN\": os.environ[\"DATABRICKS_TOKEN\"]}\n",
    "\n",
    "project.set_secrets(secrets)\n",
    "\n",
    "code = \"\"\"\n",
    "def print_kwargs(**kwargs):\n",
    "    print(f\"kwargs: {kwargs}\")\n",
    "\"\"\"\n",
    "\n",
    "function_ref = FunctionReference(\n",
    "    kind=\"databricks\",\n",
    "    code=code,\n",
    "    image=\"mlrun/mlrun\",\n",
    "    name=\"project-name\",\n",
    ")\n",
    "\n",
    "function = function_ref.to_function()\n",
    "\n",
    "for name, val in job_env.items():\n",
    "    function.spec.env.append({\"name\": name, \"value\": val})\n",
    "\n",
    "run = function.run(\n",
    "    handler=\"print_kwargs\",\n",
    "    project=\"project-name\",\n",
    "    params={\n",
    "        \"param1\": \"value1\",\n",
    "        \"param2\": \"value2\",\n",
    "    },\n",
    ")\n",
    "assert (\n",
    "    run.status.results[\"databricks_runtime_task\"][\"logs\"]\n",
    "    == \"kwargs: {'param1': 'value1', 'param2': 'value2'}\\n\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

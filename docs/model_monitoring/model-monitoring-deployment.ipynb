{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Model Monitoring (beta)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "```{note}\n",
    "The model monitoring feature is based on Iguazio's streaming technology. To enable Model Monitoring you will need to contact Iguazio.\n",
    "```\n",
    "\n",
    "## Introduction\n",
    "MLRun provides a model monitoring service that tracks the performance of models in production to help identify\n",
    "potential issues with concept drift and prediction accuracy before they impact business goals.\n",
    "Typically, Model monitoring is used by devops for tracking model performance, and by data scientists to track model drift.\n",
    "Two monitoring types are supported:\n",
    "1. Model operational performance (stream processing)\n",
    "2. Concept drift detection and accuracy monitoring (features/labels/predictions)\n",
    "\n",
    "Model Monitoring also provides warning alerts that can be sent to stakeholders for processing.\n",
    "\n",
    "The Model Monitoring feature data can be viewed using Iguazio's user interface portal or through Grafana Dashboards.\n",
    "\n",
    "## Architecture\n",
    "The Model Monitoring process flow starts by collecting operational data, converts them to vectors which are then posted to the Model Server.\n",
    "Then, the Model Server is wrapped around a machine learning model that uses a function to calculate predictions based on the available vectors.\n",
    "Next, the Model Server creates a log for the input and output of the vectors, and the entries are written to the production data stream.\n",
    "While the Model Server is processing the vectors, a Nuclio operation monitors the log of the data stream and is triggered when a new log entry is detected.\n",
    "The Nuclio function examines the log entry, processes it in to statistics which are then written to the statistics databases.\n",
    "In parallel, a scheduled MLRun runs and reads the parquet files performing the drift analysis which is then stored so that\n",
    "the user can retrieve the analysis data in the Iguazio UI or in a Grafana instance.\n",
    "\n",
    "![Architecture](./model-monitoring-data-flow.svg)\n",
    "\n",
    "## Model Endpoints Overview Dashboard\n",
    "The Overview dashboard will display the model endpoint IDs of a specific project. Only deployed models with Model Monitoring option enabled are displayed.\n",
    "Endpoint IDs are URIs used to provide access to performance data, drift detection statistics, and accuracy monitoring of a deployed model.\n",
    "\n",
    "![overview](./overview.png)\n",
    "\n",
    "The Overview screen providers details about the performance of all the deployed and monitored models within a project. You can change projects by choosing a new project from the\n",
    "**Project** dropdown. The Overview dashboard displays the number of endpoints in the project, the average predictions per second (using a 5-minute rolling average),\n",
    "the average latency (using a 1-hour rolling average), and the total error count in the project.\n",
    "\n",
    "Additional details include:\n",
    "* **Endpoint ID**&mdash;the ID of the deployed model. Use this link to drill down to the model performance and details screens.\n",
    "* **Function**&mdash;the MLRun function to access the model\n",
    "* **Model**&mdash;user defined name for the model\n",
    "* **Model Class**&mdash;the implementation class that is used by the endpoint\n",
    "* **First Request**&mdash;first request for production data\n",
    "* **Last Request**&mdash;most recent request for production data\n",
    "* **Error count**&mdash;total error count from processing production data\n",
    "* **Accuracy**&mdash;a numeric value representing the accuracy of model predictions\n",
    "* **Drift Status**&mdash;no drift (green), possible drift (yellow), drift detected (red)\n",
    "\n",
    "At the bottom of the dashboard are heat maps for the Predictions per second, Average Latency and Errors. The heat maps display data based on 15 minute intervals.\n",
    "\n",
    "Click an endpoint ID to drill down the performance details of that model.\n",
    "\n",
    "## Model Endpoint Details Dashboard\n",
    "The model endpoint details dashboard displays the real time performance data of the selected model in detail.\n",
    "Model performance data provided is rich and is used to fine tune or diagnose potential performance issues that may affect business goals.\n",
    "The data in this dashboard changes based on the selection of the project and model.\n",
    "\n",
    "This dashboard is broken down into three panes:\n",
    "\n",
    "1. [Project and model summary](#project-and-model-summary)\n",
    "2. [Analysis panes](#analysis-panes)\n",
    "   1. Overall drift analysis\n",
    "   2. Features analysis\n",
    "3. [Incoming features graph](#incoming-features-graph)\n",
    "\n",
    "![details](./details.png)\n",
    "\n",
    "### Project and Model Summary\n",
    "Use the dropdown to change the project and model. The dashboard presents the following information about the project:\n",
    "* **Endpoint ID**&mdash;the ID of the deployed model\n",
    "* **Model**&mdash;user defined name for the model\n",
    "* **Function URI**&mdash;the MLRun function to access the model\n",
    "* **Model Class**&mdash;the implementation class that is used by the endpoint\n",
    "* **Prediction/s**&mdash;the average number of predictions per second over a rolling 5-minute period\n",
    "* **Average Latency**&mdash;the average latency over a rolling 1-hour period\n",
    "* **First Request**&mdash;first request for production data\n",
    "* **Last Request**&mdash;most recent request for production data\n",
    "\n",
    "\n",
    "Use the [Performance](#model-endpoint-performance-dashboard) and [Overview](#model-endpoints-overview-dashboard) buttons view those dashboards.\n",
    "\n",
    "### Analysis Panes\n",
    "This pane is broken down into sections: Overall Drift Analysis and Features Analysis.\n",
    "Concept Drift in machine learning is a situation where the statistical properties of the target variable (what the model is trying to predict) change over time.\n",
    "In other words, the production data has changed significantly over the course of time and no longer matches the input data used to train the model.\n",
    "So, for this new data, accuracy of the model predictions is low.\n",
    "For more information see <a href=\"https://www.iguazio.com/glossary/concept-drift/\" target=\"_blank\">Concept Drift</a>.\n",
    "\n",
    "The Overall Drift Analysis pane provides performance statistics for the currently selected model. This pane includes three types of statistics:\n",
    "* **Total Variation Distance** (TVD sum and mean)&mdash;this is the statistical difference between the actual predictions, and the model's trained predictions\n",
    "* **Hellinger Distance** (sum and mean)&mdash;this is a type of f-divergence that quantifies the similarity between the actual predictions, and the model's trained predictions.\n",
    "* **Kullback–Leibler Divergence** (KLD sum and mean)&mdash;this is the measure of how the probability distribution of actual predictions is different from the second, model's trained reference probability distribution.\n",
    "\n",
    "The Features Analysis pane provides details of the drift analysis for each feature in the selected model.\n",
    "This pane includes five types of statistics:\n",
    "* **Actual** (min, mean and max)&mdash;results based on actual live data stream\n",
    "* **Expected** (min, mean and max)&mdash;results based on training data\n",
    "* **Total Variation Distance** (TVD)&mdash;this is the statistical difference between the actual predictions, and the model's trained predictions\n",
    "* **Hellinger Distance**&mdash;this is a type of f-divergence that quantifies the similarity between the actual predictions, and the model's trained predictions.\n",
    "* **Kullback–Leibler Divergence** (KLD)&mdash;this is the measure of how the probability distribution of actual preditions is different from the second, model's trained reference probability distribution.\n",
    "\n",
    "### Incoming Features Graph\n",
    "This graph displays the performance of the features that are in the selected model based on sampled data points from actual feature production data.\n",
    "The graph displays the values of the features in the model over time.\n",
    "\n",
    "## Model Endpoint Performance Dashboard\n",
    "Model endpoint performance displays performance details in graphical format.\n",
    "\n",
    "![performance](./performance.png)\n",
    "\n",
    "This dashboard is broken down into 5 graphs:\n",
    "* **Drift Measures**&mdash;the overall drift over time for each of the endpoints in the selected model\n",
    "* **Average Latency**&mdash;the average latency of the model in 5 minute intervals, for 5 minutes and 1 hour rolling windows\n",
    "* **Predictions/s**&mdash;the model predictions per second displayed in 5 second intervals for 5 minutes (rolling)\n",
    "* **Predictions Count**&mdash;the number of predictions the model makes for 5 minutes and 1 hour rolling windows\n",
    "* **Custom Metrics**&mdash;\n",
    "\n",
    "### Configuring Custom Metrics <a name=\"config-custom-metrics\"></a>\n",
    "The Custom Metrics graph can be used to view metrics that are not typically displayed on the current suite of dashboards,\n",
    "Additional metrics include:\n",
    "* TVD&mdash;\n",
    "\n",
    "## Initial Set Up (pre-requisites)\n",
    "1. Make sure you have the `mlrun-api` as a Grafana data source configured in your Grafana instance. If not,\n",
    "add it by:\n",
    "   1. Open your grafana instance.\n",
    "   2. Navigate to `Configuration -> Data Sources`.\n",
    "   3. Press `Add data source`.\n",
    "   4. Select the `SimpleJson` datasource and configure the following parameters.\n",
    "    ```\n",
    "    Name: mlrun-api\n",
    "    URL: http://mlrun-api:8080/api/grafana-proxy/model-endpoints\n",
    "    Access: Server (default)\n",
    "\n",
    "    ## Add a custom header of:\n",
    "    X-V3io-Session-Key: <YOUR ACCESS KEY>\n",
    "    ```\n",
    "    5. Press `Save & Test` for verification. A confirmation message should appear if it is working.\n",
    "\n",
    "2. Download the following monitoring dashboards:\n",
    "    1. [Overview](./dashboards/overview.json)\n",
    "    2. [Details](./dashboards/details.json)\n",
    "    3. [Performance](./dashboards/performance.json)\n",
    "\n",
    "3. Import the downloaded dashboards to your Grafana instance.\n",
    "\n",
    "Use the following code to supply training data to the model in order to utilize drift measurement.\n",
    "\n",
    "4. To allow the system to utilize drift measurement, make sure you supply the train set when logging the model in the\n",
    "   training step\n",
    "\n",
    "    ```python\n",
    "    # Log model\n",
    "    context.log_model(\n",
    "        \"model\",\n",
    "        body=dumps(model),\n",
    "        artifact_path=context.artifact_subpath(\"models\"),\n",
    "        extra_data=eval_metrics,\n",
    "        model_file=\"model.pkl\",\n",
    "        metrics=context.results,\n",
    "        training_set=<TRAIN_SET>,  # <-\n",
    "        label_cols=<LABEL_COLS>,  # <-\n",
    "        labels={\"class\": \"sklearn.linear_model.LogisticRegression\"}\n",
    "    )\n",
    "    ```\n",
    "5. When serving the model, make sure that the Nuclio function is deployed with tracking enabled by applying\n",
    "   `fn.set_tracking()` on the serving function.\n",
    "\n",
    "## Configuration\n",
    "The stream processing portion of the model monitoring can be deployed under multiple configuration options. The\n",
    "available configurations can be found under `stream.Config`. Once configured, it should be supplied as environment\n",
    "parameters to the Nuclio function by setting `fn.set_envs`\n",
    "\n",
    "```python\n",
    "project: str                        # project name\n",
    "sample_window: int                  # The sampling window for the data that flows into the TSDB and the KV\n",
    "tsdb_batching_max_events: int       # The max amount of event to batch before writing the batch to tsdb\n",
    "tsdb_batching_timeout_secs: int     # The max amount of seconds a given batch can be gathered before being emitted\n",
    "parquet_batching_max_events: int    # The max amount of event to batch before writing the batch to parquet\n",
    "parquet_batching_timeout_secs: int  # The max amount of seconds, a given batch can be gathered before being written to parquet\n",
    "aggregate_count_windows: List[str]  # List of window sizes for predictions count\n",
    "aggregate_count_period: str         # Period of predictions count windows\n",
    "aggregate_avg_windows: List[str]    # List of window sizes for average latency\n",
    "aggregate_avg_period: str           # Period of average latency windows\n",
    "v3io_access_key: str                # V3IO Access key, if not set will be taken from environment\n",
    "v3io_framesd: str                   # V3IO framesd URL, if not set will be taken from environment\n",
    "```"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Set project name\n",
    "project = \"\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Deploy Model Servers\n",
    "\n",
    "Use the following code to deploy the model servers."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "from mlrun import import_function, get_dataitem\n",
    "from mlrun import projects\n",
    "from mlrun.platforms import auto_mount\n",
    "\n",
    "proj = projects.new_project(project)\n",
    "\n",
    "get_dataitem(\"https://s3.wasabisys.com/iguazio/models/iris/model.pkl\").download(\n",
    "    \"model.pkl\")\n",
    "\n",
    "iris = load_iris()\n",
    "train_set = pd.DataFrame(iris['data'],\n",
    "                         columns=['sepal_length_cm', 'sepal_width_cm',\n",
    "                                  'petal_length_cm', 'petal_width_cm'])\n",
    "\n",
    "model_names = [\n",
    "    \"sklearn_ensemble_RandomForestClassifier\",\n",
    "    \"sklearn_linear_model_LogisticRegression\",\n",
    "    \"sklearn_ensemble_AdaBoostClassifier\"\n",
    "]\n",
    "\n",
    "serving_fn = import_function('hub://v2_model_server').apply(auto_mount())\n",
    "\n",
    "for name in model_names:\n",
    "    proj.log_model(name, model_file=\"model.pkl\", training_set=train_set)\n",
    "    serving_fn.add_model(name,\n",
    "                         model_path=f\"store://models/{project}/{name}:latest\")\n",
    "\n",
    "serving_fn.metadata.project = project\n",
    "serving_fn.set_tracking()\n",
    "serving_fn.deploy()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Deploy Stream Processing\n",
    "\n",
    "Use the following code to deploy the stream processing."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from mlrun import import_function\n",
    "from mlrun.platforms import mount_v3io\n",
    "from mlrun.runtimes import RemoteRuntime\n",
    "import json\n",
    "\n",
    "fn: RemoteRuntime = import_function(\"hub://model_monitoring_stream\")\n",
    "\n",
    "fn.add_v3io_stream_trigger(\n",
    "    stream_path=f\"projects/{project}/model-endpoints/stream\",\n",
    "    name=\"monitoring_stream_trigger\",\n",
    ")\n",
    "\n",
    "fn.set_env(\"MODEL_MONITORING_PARAMETERS\", json.dumps(\n",
    "    {\"project\": project, \"v3io_framesd\": os.environ.get(\"V3IO_FRAMESD\")}))\n",
    "\n",
    "fn.metadata.project = project\n",
    "fn.apply(mount_v3io())\n",
    "fn.deploy()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Deploy Batch Processing\n",
    "\n",
    "Use the following code to deploy batch processing."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from mlrun import import_function\n",
    "from mlrun.platforms import mount_v3io\n",
    "from mlrun.runtimes import KubejobRuntime\n",
    "\n",
    "fn: KubejobRuntime = import_function(\"hub://model_monitoring_batch\")\n",
    "fn.metadata.project = project\n",
    "fn.apply(mount_v3io())\n",
    "fn.run(name='model-monitoring-batch', schedule=\"0 */1 * * *\",\n",
    "       params={\"project\": project})"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Simulating Requests\n",
    "\n",
    "Use the following code to stimulate requests and view data in the model monitoring feature."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import json\n",
    "from time import sleep\n",
    "from random import choice, uniform\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "iris = load_iris()\n",
    "iris_data = iris['data'].tolist()\n",
    "\n",
    "while True:\n",
    "    for name in model_names:\n",
    "        data_point = choice(iris_data)\n",
    "        serving_fn.invoke(f'v2/models/{name}/infer',\n",
    "                          json.dumps({'inputs': [data_point]}))\n",
    "        sleep(uniform(0.1, 0.4))\n",
    "    sleep(uniform(0.2, 1.7))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
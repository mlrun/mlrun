{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ea7ad9f",
   "metadata": {},
   "source": [
    "(evaluating-llms)="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ac4f331-7ce4-4b99-80ad-2b5607a42c4f",
   "metadata": {},
   "source": [
    "# Evaluating LLMs with MLRun"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ce305d8-0cf4-4426-a314-9c11dd217a8f",
   "metadata": {},
   "source": [
    "Evaluating large language models (LLMs) is crucial throughout the ML lifecycle. During development, thorough evaluation enables users to refine prompts, select models, and tune hyperparameters. During production, real-time evaluation and guardrails ensure that responses from LLMs are reliable, consistent, and relevant in real-world applications.\n",
    "\n",
    "## Challenges in evaluating LLMs\n",
    "\n",
    "Evaluating Large Language Models (LLMs) comes with its own set of challenges:\n",
    "\n",
    "- **Lack of Standardization**: There is no single, universally accepted evaluation framework or metrics suite for LLMs. This makes it difficult to compare and benchmark different models across various tasks. There are a number of benchmark datasets for evaluating LLM performance such as [GSM8K](https://huggingface.co/datasets/openai/gsm8k), however these are not always representative of real-world performance.\n",
    "\n",
    "- **Complexity of Evaluation Tasks**: Many evaluation tasks are complex and multifaceted, involving multiple aspects such as factual accuracy, coherence, fluency, and relevance. Additionally, LLMs are prone to hallucination meaning that the final response may deviate from some provided, factually correct context.\n",
    "\n",
    "- **Subjectivity in Evaluation**: Evaluation metrics and tasks can be subjective, making it challenging to determine the \"ground truth\" or what constitutes a correct answer. This subjectivity can lead to varying evaluation results across different evaluators or even the same evaluator at different times.\n",
    "\n",
    "- **Human Judgement Resources**: Limited human judgement resources make large-scale evaluations using human judgements impractical. This limitation highlights the need for automated and scalable evaluation methods.\n",
    "\n",
    "## Metrics overview\n",
    "\n",
    "Open source frameworks such as [Deepeval](https://docs.confident-ai.com/docs/getting-started) offer a range of metrics to evaluate various aspects of an LLM's output. In particular, the following metrics are related to comparing the LLM's response to some provided context (like from a RAG system) to ensure the response is high quality, factually correct, and representative of the external knowledge base.\n",
    "\n",
    "This example uses the following metrics:\n",
    "- **[Answer Relevancy](https://docs.confident-ai.com/docs/metrics-answer-relevancy)**: Measures the quality of an LLM's generator by evaluating how relevant the actual output is compared to the provided input.\n",
    "- **[Faithfulness](https://docs.confident-ai.com/docs/metrics-faithfulness)**: Evaluates whether the actual output factually aligns with the contents of the retrieval context.\n",
    "- **[Contextual Precision](https://docs.confident-ai.com/docs/metrics-contextual-precision)**: Assesses the LLM's retriever by evaluating whether nodes in the retrieval context that are relevant to the given input are ranked higher than irrelevant ones.\n",
    "- **[Contextual Recall](https://docs.confident-ai.com/docs/metrics-contextual-recall)**: Measures the quality of an LLM's retriever by evaluating the extent to which the retrieval context aligns with the expected output.\n",
    "- **[Contextual Relevancy](https://docs.confident-ai.com/docs/metrics-contextual-relevancy)**: Evaluates the overall relevance of the information presented in the retrieval context for a given input.\n",
    "\n",
    "## Prerequisite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "384ed3fd-d9c7-4377-8dad-88a3b665d115",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install --upgrade deepeval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "4a33f6a1-27ae-4b40-864b-045513f4e142",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install \"protobuf<3.20\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a78c352-f98b-4104-a5b6-b95bfe01eee4",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d6fa7d03-0b6c-4e71-9956-274c2068f699",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import mlrun\n",
    "import pandas as pd\n",
    "from deepeval import evaluate\n",
    "from deepeval.metrics import (\n",
    "    AnswerRelevancyMetric,\n",
    "    ContextualPrecisionMetric,\n",
    "    ContextualRecallMetric,\n",
    "    ContextualRelevancyMetric,\n",
    "    FaithfulnessMetric,\n",
    ")\n",
    "from deepeval.models.base_model import DeepEvalBaseLLM\n",
    "from deepeval.test_case import LLMTestCase\n",
    "from mlrun.utils import create_class\n",
    "\n",
    "# OpenAI\n",
    "OPENAI_API_KEY = \"\"\n",
    "OPENAI_BASE_URL = \"https://api.openai.com/v1\"\n",
    "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY\n",
    "os.environ[\"OPENAI_BASE_URL\"] = OPENAI_BASE_URL\n",
    "OPENAI_MODEL = \"gpt-3.5-turbo-0125\"\n",
    "\n",
    "# Ollama\n",
    "OLLAMA_URL = \"http://ollama.default.svc.cluster.local:11434\"\n",
    "OLLAMA_MODEL = \"llama3\"\n",
    "\n",
    "\n",
    "# Custom langchain class for deepeval\n",
    "class DeepEvalLangchainLLM(DeepEvalBaseLLM):\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "\n",
    "    def load_model(self):\n",
    "        return self.model\n",
    "\n",
    "    def generate(self, prompt: str) -> str:\n",
    "        chat_model = self.load_model()\n",
    "        return chat_model.invoke(prompt).content\n",
    "\n",
    "    async def a_generate(self, prompt: str) -> str:\n",
    "        chat_model = self.load_model()\n",
    "        res = await chat_model.ainvoke(prompt)\n",
    "        return res.content\n",
    "\n",
    "    def get_model_name(self):\n",
    "        try:\n",
    "            return self.model.model\n",
    "        except AttributeError:\n",
    "            pass\n",
    "        try:\n",
    "            return self.model.model_name\n",
    "        except AttributeError:\n",
    "            pass\n",
    "        return \"Custom Langchain Model\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f264a56-f226-46c2-8425-47b4f60c953e",
   "metadata": {},
   "source": [
    "## Select OpenAI or Ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "837bffe3-281a-4c01-9b6d-9cb31a5639bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using mode: OPENAI\n",
      "\n",
      "LLM model: gpt-3.5-turbo-0125\n"
     ]
    }
   ],
   "source": [
    "MODE = \"openai\"  # also supports ollama\n",
    "\n",
    "if MODE == \"openai\":\n",
    "    model = OPENAI_MODEL\n",
    "\n",
    "elif MODE == \"ollama\":\n",
    "    llm_class = \"langchain_community.chat_models.ChatOllama\"\n",
    "    llm_kwargs = {\"model\": OLLAMA_MODEL, \"base_url\": OLLAMA_URL}\n",
    "    llm = create_class(llm_class)(**llm_kwargs)\n",
    "    model = DeepEvalLangchainLLM(model=llm)\n",
    "else:\n",
    "    raise ValueError(f\"Mode {MODE} not supported\")\n",
    "\n",
    "print(f\"Using mode: {MODE.upper()}\\n\")\n",
    "print(f\"LLM model: {model}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8efdbea5-733f-4828-890a-fa7a6d4eaeb9",
   "metadata": {},
   "source": [
    "## Example evaluation task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "906daacb-4dd8-471b-a324-9304eb77a60d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7da9f96530e04d9dafbecd3960e90b35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating test cases...\n",
      "Event loop is already running. Applying nest_asyncio patch to allow async execution...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "\n",
      "Metrics Summary\n",
      "\n",
      "  - ❌ Faithfulness (score: 0.0, threshold: 0.5, strict: False, evaluation model: gpt-3.5-turbo-0125, reason: The score is 0.00 because the actual output directly contradicts the retrieval context by stating you are allowed to stay for 30 days after completing your degree instead of the correct 60 days., error: None)\n",
      "\n",
      "For test case:\n",
      "\n",
      "  - input: I'm on an F-1 visa, gow long can I stay in the US after graduation?\n",
      "  - actual output: You can stay up to 30 days after completing your degree.\n",
      "  - expected output: You can stay up to 60 days after completing your degree.\n",
      "  - context: None\n",
      "  - retrieval context: ['If you are in the U.S. on an F-1 visa, you are allowed to stay for 60 days after completing\\n        your degree, unless you have applied for and been approved to participate in OPT.']\n",
      "\n",
      "======================================================================\n",
      "\n",
      "Overall Metric Pass Rates\n",
      "\n",
      "FaithfulnessMetric: 0.00% pass rate\n",
      "\n",
      "======================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "timeout has no effect in blocking mode\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">✅ Tests finished! Run <span style=\"color: #008000; text-decoration-color: #008000\">\"deepeval login\"</span> to view evaluation results on the web.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "✅ Tests finished! Run \u001b[32m\"deepeval login\"\u001b[0m to view evaluation results on the web.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_case = LLMTestCase(\n",
    "    input=\"I'm on an F-1 visa, gow long can I stay in the US after graduation?\",\n",
    "    actual_output=\"You can stay up to 30 days after completing your degree.\",\n",
    "    expected_output=\"You can stay up to 60 days after completing your degree.\",\n",
    "    retrieval_context=[\n",
    "        \"\"\"If you are in the U.S. on an F-1 visa, you are allowed to stay for 60 days after completing\n",
    "        your degree, unless you have applied for and been approved to participate in OPT.\"\"\"\n",
    "    ],\n",
    ")\n",
    "\n",
    "faithfulness = FaithfulnessMetric(model=model)\n",
    "\n",
    "results = evaluate(test_cases=[test_case], metrics=[faithfulness])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3318aca-2273-457f-903c-b6b6d066aca3",
   "metadata": {},
   "source": [
    "## Create an evaluation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "6bea9fe2-f26d-4eaa-877d-bed0e8c34ecf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting evaluate_llm.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile evaluate_llm.py\n",
    "\n",
    "import os\n",
    "\n",
    "import mlrun\n",
    "import pandas as pd\n",
    "from deepeval import evaluate\n",
    "from deepeval.models.base_model import DeepEvalBaseLLM\n",
    "from deepeval.test_case import LLMTestCase\n",
    "from mlrun.utils import create_class\n",
    "\n",
    "@mlrun.handler(outputs=[\"evaluation\"])\n",
    "def evaluate_llm(\n",
    "    test_cases: list[dict],\n",
    "    metrics: list[str],\n",
    "    model: str\n",
    "):\n",
    "    results = evaluate(\n",
    "        test_cases=[LLMTestCase(**t) for t in test_cases],\n",
    "        metrics=[create_class(m)(model=model) for m in metrics]\n",
    "    )\n",
    "    \n",
    "    rows = []\n",
    "    for i, result in enumerate(results):\n",
    "        for metric in result.metrics:\n",
    "            result_dict = {\n",
    "                \"test\" : f\"test_case_{i}\",\n",
    "                \"actual_output\" : result.actual_output,\n",
    "                \"expected_output\" : result.expected_output,\n",
    "                \"context\" : result.context,\n",
    "                \"retrieval_context\" : result.retrieval_context,\n",
    "                \"user_input\" : result.input,\n",
    "                \"test_success\" : result.success,\n",
    "                \"metric_success\" : metric.success,\n",
    "                \"metric\" : metric.__name__,\n",
    "                \"evaluation_model\" : metric.evaluation_model,\n",
    "                \"metric_score\" : metric.score,\n",
    "                \"metric_reason\" : metric.reason,\n",
    "                \"evaluation_cost\": metric.evaluation_cost,\n",
    "                \"metric_threshold\" : metric.threshold,\n",
    "                \"metric_error\" : metric.error\n",
    "            }\n",
    "            rows.append(result_dict)\n",
    "    df = pd.DataFrame(rows)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "f3ff847c-332b-4455-8dd2-74440f36c7be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> 2024-06-12 16:01:54,211 [info] Project loaded successfully: {'project_name': 'evaluate'}\n"
     ]
    }
   ],
   "source": [
    "project = mlrun.get_or_create_project(\"evaluate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "c3ad6846-ec67-4bc5-8c4e-0a774176a0d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<mlrun.runtimes.kubejob.KubejobRuntime at 0x7f461c116b80>"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluation_fn = project.set_function(\n",
    "    name=\"evaluate-llm\",\n",
    "    func=\"evaluate_llm.py\",\n",
    "    kind=\"job\",\n",
    "    image=\"mlrun/mlrun\",\n",
    "    handler=\"evaluate_llm\",\n",
    ")\n",
    "\n",
    "# Only relevant for OpenAI\n",
    "evaluation_fn.set_envs(\n",
    "    {\"OPENAI_API_KEY\": OPENAI_API_KEY, \"OPENAI_BASE_URL\": OPENAI_BASE_URL}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7e940fa-ca0d-458e-8cf7-0f7288df4ea3",
   "metadata": {},
   "source": [
    "## Run an evaluation job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "45b1e281-195c-40c5-b4d7-d5f128a77037",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> 2024-06-12 16:24:21,581 [info] Storing function: {'name': 'evaluate-llm-evaluate-llm', 'uid': '795196166bf64eb896d1501109bc197e', 'db': 'http://mlrun-api:8080'}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0964c0ebc6ef488ab561adc1a35c776a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating test cases...\n",
      "Event loop is already running. Applying nest_asyncio patch to allow async execution...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "160fb05324214316a5494107671d71e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "timeout has no effect in blocking mode\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "\n",
      "Metrics Summary\n",
      "\n",
      "  - ✅ Answer Relevancy (score: 1.0, threshold: 0.5, strict: False, evaluation model: gpt-3.5-turbo-0125, reason: The score is 1.00 because the response provided directly answers the question with relevant information., error: None)\n",
      "  - ❌ Faithfulness (score: 0.0, threshold: 0.5, strict: False, evaluation model: gpt-3.5-turbo-0125, reason: The score is 0.00 because the actual output does not align with the information presented in the retrieval context, indicating a lack of faithfulness., error: None)\n",
      "  - ✅ Contextual Precision (score: 1.0, threshold: 0.5, strict: False, evaluation model: gpt-3.5-turbo-0125, reason: The score is perfect because the relevant context directly answers the question, providing a clear and concise response., error: None)\n",
      "  - ✅ Contextual Recall (score: 1.0, threshold: 0.5, strict: False, evaluation model: gpt-3.5-turbo-0125, reason: The score is 1.00 because the sentence perfectly matches the information retrieved from the 1st node in the retrieval context., error: None)\n",
      "  - ✅ Contextual Relevancy (score: 1.0, threshold: 0.5, strict: False, evaluation model: gpt-3.5-turbo-0125, reason: The score is 1.00 because the input directly relates to the topic of F-1 visa duration after graduation., error: None)\n",
      "\n",
      "For test case:\n",
      "\n",
      "  - input: I'm on an F-1 visa, gow long can I stay in the US after graduation?\n",
      "  - actual output: You can stay up to 30 days after completing your degree.\n",
      "  - expected output: You can stay up to 60 days after completing your degree.\n",
      "  - context: None\n",
      "  - retrieval context: ['If you are in the U.S. on an F-1 visa, you are allowed to stay for 60 days after completing\\n                    your degree, unless you have applied for and been approved to participate in OPT.']\n",
      "\n",
      "======================================================================\n",
      "\n",
      "Metrics Summary\n",
      "\n",
      "  - ✅ Answer Relevancy (score: 1.0, threshold: 0.5, strict: False, evaluation model: gpt-3.5-turbo-0125, reason: The score is 1.00 because the answer is completely relevant to the input., error: None)\n",
      "  - ❌ Faithfulness (score: 0.3333333333333333, threshold: 0.5, strict: False, evaluation model: gpt-3.5-turbo-0125, reason: The score is 0.33 because the actual output includes information about MLRun enabling the development, training, deployment, and management of machine learning models in a serverless environment, as well as providing tools and APIs for building, testing, and deploying model serving functions, which were not mentioned in the retrieval context., error: None)\n",
      "  - ❌ Contextual Precision (score: 0, threshold: 0.5, strict: False, evaluation model: gpt-3.5-turbo-0125, reason: The score is 0.00 because the irrelevant nodes are ranked higher than the relevant nodes, as the first node does not directly address the benefits of MLRun and the second node discusses MLRun's features instead of its benefits in reducing engineering efforts, time to production, and computation resources., error: None)\n",
      "  - ✅ Contextual Recall (score: 1.0, threshold: 0.5, strict: False, evaluation model: gpt-3.5-turbo-0125, reason: The score is 1.00 because all the expected sentences can be directly attributed to the nodes in the retrieval context, indicating perfect contextual recall., error: None)\n",
      "  - ✅ Contextual Relevancy (score: 1.0, threshold: 0.5, strict: False, evaluation model: gpt-3.5-turbo-0125, reason: The score is 1.00 because the input directly asks about the benefits of MLRun., error: None)\n",
      "\n",
      "For test case:\n",
      "\n",
      "  - input: What are some benefits of MLRun?\n",
      "  - actual output: MLRun is an MLOps orchestration framework that enables you to develop, train, deploy, and manage machine learning models in a serverless environment. It provides a set of tools and APIs for building, testing, and deploying model serving functions.\n",
      "  - expected output: MLRun is an open MLOps platform for quickly building and managing continuous ML applications across their lifecycle. MLRun integrates into your development and CI/CD environment and automates the delivery of production data, ML pipelines, and online applications. MLRun significantly reduces engineering efforts, time to production, and computation resources. With MLRun, you can choose any IDE on your local machine or on the cloud. MLRun breaks the silos between data, ML, software, and DevOps/MLOps teams, enabling collaboration and fast continuous improvements.\n",
      "  - context: None\n",
      "  - retrieval context: ['Instead of a siloed, complex, and manual process, MLRun enables production pipeline design using a modular strategy, where the different parts contribute to a continuous, automated, and far simpler path from research and development to scalable production pipelines without refactoring code, adding glue logic, or spending significant efforts on data and ML engineering.', 'MLRun uses Serverless Function technology: write the code once, using your preferred development environment and simple \"local\" semantics, and then run it as-is on different platforms and at scale. MLRun automates the build process, execution, data movement, scaling, versioning, parameterization, output tracking, CI/CD integration, deployment to production, monitoring, and more.']\n",
      "\n",
      "======================================================================\n",
      "\n",
      "Overall Metric Pass Rates\n",
      "\n",
      "AnswerRelevancyMetric: 100.00% pass rate\n",
      "FaithfulnessMetric: 0.00% pass rate\n",
      "ContextualPrecisionMetric: 50.00% pass rate\n",
      "ContextualRecallMetric: 100.00% pass rate\n",
      "ContextualRelevancyMetric: 100.00% pass rate\n",
      "\n",
      "======================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "timeout has no effect in blocking mode\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">✅ Tests finished! Run <span style=\"color: #008000; text-decoration-color: #008000\">\"deepeval login\"</span> to view evaluation results on the web.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "✅ Tests finished! Run \u001b[32m\"deepeval login\"\u001b[0m to view evaluation results on the web.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Converting input from bool to <class 'numpy.uint8'> for compatibility.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".dictlist {\n",
       "  background-color: #4EC64B;\n",
       "  text-align: center;\n",
       "  margin: 4px;\n",
       "  border-radius: 3px; padding: 0px 3px 1px 3px; display: inline-block;}\n",
       ".artifact {\n",
       "  cursor: pointer;\n",
       "  background-color: #4EC64B;\n",
       "  text-align: left;\n",
       "  margin: 4px; border-radius: 3px; padding: 0px 3px 1px 3px; display: inline-block;\n",
       "}\n",
       "div.block.hidden {\n",
       "  display: none;\n",
       "}\n",
       ".clickable {\n",
       "  cursor: pointer;\n",
       "}\n",
       ".ellipsis {\n",
       "  display: inline-block;\n",
       "  max-width: 60px;\n",
       "  white-space: nowrap;\n",
       "  overflow: hidden;\n",
       "  text-overflow: ellipsis;\n",
       "}\n",
       ".master-wrapper {\n",
       "  display: flex;\n",
       "  flex-flow: row nowrap;\n",
       "  justify-content: flex-start;\n",
       "  align-items: stretch;\n",
       "}\n",
       ".master-tbl {\n",
       "  flex: 3\n",
       "}\n",
       ".master-wrapper > div {\n",
       "  margin: 4px;\n",
       "  padding: 10px;\n",
       "}\n",
       "iframe.fileview {\n",
       "  border: 0 none;\n",
       "  height: 100%;\n",
       "  width: 100%;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       ".pane-header-title {\n",
       "  width: 80%;\n",
       "  font-weight: 500;\n",
       "}\n",
       ".pane-header {\n",
       "  line-height: 1;\n",
       "  background-color: #4EC64B;\n",
       "  padding: 3px;\n",
       "}\n",
       ".pane-header .close {\n",
       "  font-size: 20px;\n",
       "  font-weight: 700;\n",
       "  float: right;\n",
       "  margin-top: -5px;\n",
       "}\n",
       ".master-wrapper .right-pane {\n",
       "  border: 1px inset silver;\n",
       "  width: 40%;\n",
       "  min-height: 300px;\n",
       "  flex: 3\n",
       "  min-width: 500px;\n",
       "}\n",
       ".master-wrapper * {\n",
       "  box-sizing: border-box;\n",
       "}\n",
       "</style><script>\n",
       "function copyToClipboard(fld) {\n",
       "    if (document.queryCommandSupported && document.queryCommandSupported('copy')) {\n",
       "        var textarea = document.createElement('textarea');\n",
       "        textarea.textContent = fld.innerHTML;\n",
       "        textarea.style.position = 'fixed';\n",
       "        document.body.appendChild(textarea);\n",
       "        textarea.select();\n",
       "\n",
       "        try {\n",
       "            return document.execCommand('copy'); // Security exception may be thrown by some browsers.\n",
       "        } catch (ex) {\n",
       "\n",
       "        } finally {\n",
       "            document.body.removeChild(textarea);\n",
       "        }\n",
       "    }\n",
       "}\n",
       "function expandPanel(el) {\n",
       "  const panelName = \"#\" + el.getAttribute('paneName');\n",
       "  console.log(el.title);\n",
       "\n",
       "  document.querySelector(panelName + \"-title\").innerHTML = el.title\n",
       "  iframe = document.querySelector(panelName + \"-body\");\n",
       "\n",
       "  const tblcss = `<style> body { font-family: Arial, Helvetica, sans-serif;}\n",
       "    #csv { margin-bottom: 15px; }\n",
       "    #csv table { border-collapse: collapse;}\n",
       "    #csv table td { padding: 4px 8px; border: 1px solid silver;} </style>`;\n",
       "\n",
       "  function csvToHtmlTable(str) {\n",
       "    return '<div id=\"csv\"><table><tr><td>' +  str.replace(/[\\n\\r]+$/g, '').replace(/[\\n\\r]+/g, '</td></tr><tr><td>')\n",
       "      .replace(/,/g, '</td><td>') + '</td></tr></table></div>';\n",
       "  }\n",
       "\n",
       "  function reqListener () {\n",
       "    if (el.title.endsWith(\".csv\")) {\n",
       "      iframe.setAttribute(\"srcdoc\", tblcss + csvToHtmlTable(this.responseText));\n",
       "    } else {\n",
       "      iframe.setAttribute(\"srcdoc\", this.responseText);\n",
       "    }\n",
       "    console.log(this.responseText);\n",
       "  }\n",
       "\n",
       "  const oReq = new XMLHttpRequest();\n",
       "  oReq.addEventListener(\"load\", reqListener);\n",
       "  oReq.open(\"GET\", el.title);\n",
       "  oReq.send();\n",
       "\n",
       "\n",
       "  //iframe.src = el.title;\n",
       "  const resultPane = document.querySelector(panelName + \"-pane\");\n",
       "  if (resultPane.classList.contains(\"hidden\")) {\n",
       "    resultPane.classList.remove(\"hidden\");\n",
       "  }\n",
       "}\n",
       "function closePanel(el) {\n",
       "  const panelName = \"#\" + el.getAttribute('paneName')\n",
       "  const resultPane = document.querySelector(panelName + \"-pane\");\n",
       "  if (!resultPane.classList.contains(\"hidden\")) {\n",
       "    resultPane.classList.add(\"hidden\");\n",
       "  }\n",
       "}\n",
       "\n",
       "</script>\n",
       "<div class=\"master-wrapper\">\n",
       "  <div class=\"block master-tbl\"><div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>project</th>\n",
       "      <th>uid</th>\n",
       "      <th>iter</th>\n",
       "      <th>start</th>\n",
       "      <th>state</th>\n",
       "      <th>name</th>\n",
       "      <th>labels</th>\n",
       "      <th>inputs</th>\n",
       "      <th>parameters</th>\n",
       "      <th>results</th>\n",
       "      <th>artifacts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>evaluate</td>\n",
       "      <td><div title=\"795196166bf64eb896d1501109bc197e\"><a href=\"https://dashboard.default-tenant.app.cst-355.iguazio-cd1.com/mlprojects/evaluate/jobs/monitor/795196166bf64eb896d1501109bc197e/overview\" target=\"_blank\" >...09bc197e</a></div></td>\n",
       "      <td>0</td>\n",
       "      <td>Jun 12 16:24:21</td>\n",
       "      <td>completed</td>\n",
       "      <td>evaluate-llm-evaluate-llm</td>\n",
       "      <td><div class=\"dictlist\">v3io_user=nick</div><div class=\"dictlist\">kind=local</div><div class=\"dictlist\">owner=nick</div><div class=\"dictlist\">host=jupyter-nick-9dccd9cf6-kp69z</div></td>\n",
       "      <td></td>\n",
       "      <td><div class=\"dictlist\">test_cases=[{'input': \"I'm on an F-1 visa, gow long can I stay in the US after graduation?\", 'actual_output': 'You can stay up to 30 days after completing your degree.', 'expected_output': 'You can stay up to 60 days after completing your degree.', 'retrieval_context': ['If you are in the U.S. on an F-1 visa, you are allowed to stay for 60 days after completing\\n                    your degree, unless you have applied for and been approved to participate in OPT.']}, {'input': 'What are some benefits of MLRun?', 'actual_output': 'MLRun is an MLOps orchestration framework that enables you to develop, train, deploy, and manage machine learning models in a serverless environment. It provides a set of tools and APIs for building, testing, and deploying model serving functions.', 'expected_output': 'MLRun is an open MLOps platform for quickly building and managing continuous ML applications across their lifecycle. MLRun integrates into your development and CI/CD environment and automates the delivery of production data, ML pipelines, and online applications. MLRun significantly reduces engineering efforts, time to production, and computation resources. With MLRun, you can choose any IDE on your local machine or on the cloud. MLRun breaks the silos between data, ML, software, and DevOps/MLOps teams, enabling collaboration and fast continuous improvements.', 'retrieval_context': ['Instead of a siloed, complex, and manual process, MLRun enables production pipeline design using a modular strategy, where the different parts contribute to a continuous, automated, and far simpler path from research and development to scalable production pipelines without refactoring code, adding glue logic, or spending significant efforts on data and ML engineering.', 'MLRun uses Serverless Function technology: write the code once, using your preferred development environment and simple \"local\" semantics, and then run it as-is on different platforms and at scale. MLRun automates the build process, execution, data movement, scaling, versioning, parameterization, output tracking, CI/CD integration, deployment to production, monitoring, and more.']}]</div><div class=\"dictlist\">metrics=['deepeval.metrics.AnswerRelevancyMetric', 'deepeval.metrics.FaithfulnessMetric', 'deepeval.metrics.ContextualPrecisionMetric', 'deepeval.metrics.ContextualRecallMetric', 'deepeval.metrics.ContextualRelevancyMetric']</div><div class=\"dictlist\">model=gpt-3.5-turbo-0125</div></td>\n",
       "      <td></td>\n",
       "      <td><div title=\"v3io:///projects/evaluate/artifacts/evaluate-llm-evaluate-llm/0/evaluation.parquet\">evaluation</div></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div></div>\n",
       "  <div id=\"result249053b5-pane\" class=\"right-pane block hidden\">\n",
       "    <div class=\"pane-header\">\n",
       "      <span id=\"result249053b5-title\" class=\"pane-header-title\">Title</span>\n",
       "      <span onclick=\"closePanel(this)\" paneName=\"result249053b5\" class=\"close clickable\">&times;</span>\n",
       "    </div>\n",
       "    <iframe class=\"fileview\" id=\"result249053b5-body\"></iframe>\n",
       "  </div>\n",
       "</div>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<b> > to track results use the .show() or .logs() methods  or <a href=\"https://dashboard.default-tenant.app.cst-355.iguazio-cd1.com/mlprojects/evaluate/jobs/monitor/795196166bf64eb896d1501109bc197e/overview\" target=\"_blank\">click here</a> to open in UI</b>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> 2024-06-12 16:24:37,013 [info] Run execution finished: {'status': 'completed', 'name': 'evaluate-llm-evaluate-llm'}\n"
     ]
    }
   ],
   "source": [
    "evaluation_run = project.run_function(\n",
    "    evaluation_fn,\n",
    "    params={\n",
    "        \"test_cases\": [\n",
    "            dict(\n",
    "                input=\"I'm on an F-1 visa, gow long can I stay in the US after graduation?\",\n",
    "                actual_output=\"You can stay up to 30 days after completing your degree.\",\n",
    "                expected_output=\"You can stay up to 60 days after completing your degree.\",\n",
    "                retrieval_context=[\n",
    "                    \"\"\"If you are in the U.S. on an F-1 visa, you are allowed to stay for 60 days after completing\n",
    "                    your degree, unless you have applied for and been approved to participate in OPT.\"\"\"\n",
    "                ],\n",
    "            ),\n",
    "            dict(\n",
    "                input=\"What are some benefits of MLRun?\",\n",
    "                actual_output=\"MLRun is an MLOps orchestration framework that enables you to develop, train, deploy, and manage machine learning models in a serverless environment. It provides a set of tools and APIs for building, testing, and deploying model serving functions.\",\n",
    "                expected_output=\"MLRun is an open MLOps platform for quickly building and managing continuous ML applications across their lifecycle. MLRun integrates into your development and CI/CD environment and automates the delivery of production data, ML pipelines, and online applications. MLRun significantly reduces engineering efforts, time to production, and computation resources. With MLRun, you can choose any IDE on your local machine or on the cloud. MLRun breaks the silos between data, ML, software, and DevOps/MLOps teams, enabling collaboration and fast continuous improvements.\",\n",
    "                retrieval_context=[\n",
    "                    \"\"\"Instead of a siloed, complex, and manual process, MLRun enables production pipeline design using a modular strategy, where the different parts contribute to a continuous, automated, and far simpler path from research and development to scalable production pipelines without refactoring code, adding glue logic, or spending significant efforts on data and ML engineering.\"\"\",\n",
    "                    \"\"\"MLRun uses Serverless Function technology: write the code once, using your preferred development environment and simple \"local\" semantics, and then run it as-is on different platforms and at scale. MLRun automates the build process, execution, data movement, scaling, versioning, parameterization, output tracking, CI/CD integration, deployment to production, monitoring, and more.\"\"\",\n",
    "                ],\n",
    "            ),\n",
    "        ],\n",
    "        \"metrics\": [\n",
    "            \"deepeval.metrics.AnswerRelevancyMetric\",\n",
    "            \"deepeval.metrics.FaithfulnessMetric\",\n",
    "            \"deepeval.metrics.ContextualPrecisionMetric\",\n",
    "            \"deepeval.metrics.ContextualRecallMetric\",\n",
    "            \"deepeval.metrics.ContextualRelevancyMetric\",\n",
    "        ],\n",
    "        \"model\": OPENAI_MODEL,\n",
    "    },\n",
    "    local=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15df7c97-449f-47f2-95a1-96fe2df81e8c",
   "metadata": {
    "tags": []
   },
   "source": [
    "## View the logged output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "7ee909d8-89b6-41da-88a9-64ccc8ecc5e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>test</th>\n",
       "      <th>actual_output</th>\n",
       "      <th>expected_output</th>\n",
       "      <th>context</th>\n",
       "      <th>retrieval_context</th>\n",
       "      <th>user_input</th>\n",
       "      <th>test_success</th>\n",
       "      <th>metric_success</th>\n",
       "      <th>metric</th>\n",
       "      <th>evaluation_model</th>\n",
       "      <th>metric_score</th>\n",
       "      <th>metric_reason</th>\n",
       "      <th>evaluation_cost</th>\n",
       "      <th>metric_threshold</th>\n",
       "      <th>metric_error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>test_case_0</td>\n",
       "      <td>You can stay up to 30 days after completing yo...</td>\n",
       "      <td>You can stay up to 60 days after completing yo...</td>\n",
       "      <td>None</td>\n",
       "      <td>[If you are in the U.S. on an F-1 visa, you ar...</td>\n",
       "      <td>I'm on an F-1 visa, gow long can I stay in the...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>Answer Relevancy</td>\n",
       "      <td>gpt-3.5-turbo-0125</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>The score is 1.00 because the response provide...</td>\n",
       "      <td>0.000485</td>\n",
       "      <td>0.5</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>test_case_0</td>\n",
       "      <td>You can stay up to 30 days after completing yo...</td>\n",
       "      <td>You can stay up to 60 days after completing yo...</td>\n",
       "      <td>None</td>\n",
       "      <td>[If you are in the U.S. on an F-1 visa, you ar...</td>\n",
       "      <td>I'm on an F-1 visa, gow long can I stay in the...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Faithfulness</td>\n",
       "      <td>gpt-3.5-turbo-0125</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>The score is 0.00 because the actual output do...</td>\n",
       "      <td>0.000917</td>\n",
       "      <td>0.5</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>test_case_0</td>\n",
       "      <td>You can stay up to 30 days after completing yo...</td>\n",
       "      <td>You can stay up to 60 days after completing yo...</td>\n",
       "      <td>None</td>\n",
       "      <td>[If you are in the U.S. on an F-1 visa, you ar...</td>\n",
       "      <td>I'm on an F-1 visa, gow long can I stay in the...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>Contextual Precision</td>\n",
       "      <td>gpt-3.5-turbo-0125</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>The score is perfect because the relevant cont...</td>\n",
       "      <td>0.000559</td>\n",
       "      <td>0.5</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>test_case_0</td>\n",
       "      <td>You can stay up to 30 days after completing yo...</td>\n",
       "      <td>You can stay up to 60 days after completing yo...</td>\n",
       "      <td>None</td>\n",
       "      <td>[If you are in the U.S. on an F-1 visa, you ar...</td>\n",
       "      <td>I'm on an F-1 visa, gow long can I stay in the...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>Contextual Recall</td>\n",
       "      <td>gpt-3.5-turbo-0125</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>The score is 1.00 because the sentence perfect...</td>\n",
       "      <td>0.000469</td>\n",
       "      <td>0.5</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>test_case_0</td>\n",
       "      <td>You can stay up to 30 days after completing yo...</td>\n",
       "      <td>You can stay up to 60 days after completing yo...</td>\n",
       "      <td>None</td>\n",
       "      <td>[If you are in the U.S. on an F-1 visa, you ar...</td>\n",
       "      <td>I'm on an F-1 visa, gow long can I stay in the...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>Contextual Relevancy</td>\n",
       "      <td>gpt-3.5-turbo-0125</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>The score is 1.00 because the input directly r...</td>\n",
       "      <td>0.000150</td>\n",
       "      <td>0.5</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>test_case_1</td>\n",
       "      <td>MLRun is an MLOps orchestration framework that...</td>\n",
       "      <td>MLRun is an open MLOps platform for quickly bu...</td>\n",
       "      <td>None</td>\n",
       "      <td>[Instead of a siloed, complex, and manual proc...</td>\n",
       "      <td>What are some benefits of MLRun?</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>Answer Relevancy</td>\n",
       "      <td>gpt-3.5-turbo-0125</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>The score is 1.00 because the answer is comple...</td>\n",
       "      <td>0.000607</td>\n",
       "      <td>0.5</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>test_case_1</td>\n",
       "      <td>MLRun is an MLOps orchestration framework that...</td>\n",
       "      <td>MLRun is an open MLOps platform for quickly bu...</td>\n",
       "      <td>None</td>\n",
       "      <td>[Instead of a siloed, complex, and manual proc...</td>\n",
       "      <td>What are some benefits of MLRun?</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Faithfulness</td>\n",
       "      <td>gpt-3.5-turbo-0125</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>The score is 0.33 because the actual output in...</td>\n",
       "      <td>0.001351</td>\n",
       "      <td>0.5</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>test_case_1</td>\n",
       "      <td>MLRun is an MLOps orchestration framework that...</td>\n",
       "      <td>MLRun is an open MLOps platform for quickly bu...</td>\n",
       "      <td>None</td>\n",
       "      <td>[Instead of a siloed, complex, and manual proc...</td>\n",
       "      <td>What are some benefits of MLRun?</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Contextual Precision</td>\n",
       "      <td>gpt-3.5-turbo-0125</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>The score is 0.00 because the irrelevant nodes...</td>\n",
       "      <td>0.000776</td>\n",
       "      <td>0.5</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>test_case_1</td>\n",
       "      <td>MLRun is an MLOps orchestration framework that...</td>\n",
       "      <td>MLRun is an open MLOps platform for quickly bu...</td>\n",
       "      <td>None</td>\n",
       "      <td>[Instead of a siloed, complex, and manual proc...</td>\n",
       "      <td>What are some benefits of MLRun?</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>Contextual Recall</td>\n",
       "      <td>gpt-3.5-turbo-0125</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>The score is 1.00 because all the expected sen...</td>\n",
       "      <td>0.001079</td>\n",
       "      <td>0.5</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>test_case_1</td>\n",
       "      <td>MLRun is an MLOps orchestration framework that...</td>\n",
       "      <td>MLRun is an open MLOps platform for quickly bu...</td>\n",
       "      <td>None</td>\n",
       "      <td>[Instead of a siloed, complex, and manual proc...</td>\n",
       "      <td>What are some benefits of MLRun?</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>Contextual Relevancy</td>\n",
       "      <td>gpt-3.5-turbo-0125</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>The score is 1.00 because the input directly a...</td>\n",
       "      <td>0.000136</td>\n",
       "      <td>0.5</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          test                                      actual_output  \\\n",
       "0  test_case_0  You can stay up to 30 days after completing yo...   \n",
       "1  test_case_0  You can stay up to 30 days after completing yo...   \n",
       "2  test_case_0  You can stay up to 30 days after completing yo...   \n",
       "3  test_case_0  You can stay up to 30 days after completing yo...   \n",
       "4  test_case_0  You can stay up to 30 days after completing yo...   \n",
       "5  test_case_1  MLRun is an MLOps orchestration framework that...   \n",
       "6  test_case_1  MLRun is an MLOps orchestration framework that...   \n",
       "7  test_case_1  MLRun is an MLOps orchestration framework that...   \n",
       "8  test_case_1  MLRun is an MLOps orchestration framework that...   \n",
       "9  test_case_1  MLRun is an MLOps orchestration framework that...   \n",
       "\n",
       "                                     expected_output context  \\\n",
       "0  You can stay up to 60 days after completing yo...    None   \n",
       "1  You can stay up to 60 days after completing yo...    None   \n",
       "2  You can stay up to 60 days after completing yo...    None   \n",
       "3  You can stay up to 60 days after completing yo...    None   \n",
       "4  You can stay up to 60 days after completing yo...    None   \n",
       "5  MLRun is an open MLOps platform for quickly bu...    None   \n",
       "6  MLRun is an open MLOps platform for quickly bu...    None   \n",
       "7  MLRun is an open MLOps platform for quickly bu...    None   \n",
       "8  MLRun is an open MLOps platform for quickly bu...    None   \n",
       "9  MLRun is an open MLOps platform for quickly bu...    None   \n",
       "\n",
       "                                   retrieval_context  \\\n",
       "0  [If you are in the U.S. on an F-1 visa, you ar...   \n",
       "1  [If you are in the U.S. on an F-1 visa, you ar...   \n",
       "2  [If you are in the U.S. on an F-1 visa, you ar...   \n",
       "3  [If you are in the U.S. on an F-1 visa, you ar...   \n",
       "4  [If you are in the U.S. on an F-1 visa, you ar...   \n",
       "5  [Instead of a siloed, complex, and manual proc...   \n",
       "6  [Instead of a siloed, complex, and manual proc...   \n",
       "7  [Instead of a siloed, complex, and manual proc...   \n",
       "8  [Instead of a siloed, complex, and manual proc...   \n",
       "9  [Instead of a siloed, complex, and manual proc...   \n",
       "\n",
       "                                          user_input  test_success  \\\n",
       "0  I'm on an F-1 visa, gow long can I stay in the...         False   \n",
       "1  I'm on an F-1 visa, gow long can I stay in the...         False   \n",
       "2  I'm on an F-1 visa, gow long can I stay in the...         False   \n",
       "3  I'm on an F-1 visa, gow long can I stay in the...         False   \n",
       "4  I'm on an F-1 visa, gow long can I stay in the...         False   \n",
       "5                   What are some benefits of MLRun?         False   \n",
       "6                   What are some benefits of MLRun?         False   \n",
       "7                   What are some benefits of MLRun?         False   \n",
       "8                   What are some benefits of MLRun?         False   \n",
       "9                   What are some benefits of MLRun?         False   \n",
       "\n",
       "   metric_success                metric    evaluation_model  metric_score  \\\n",
       "0            True      Answer Relevancy  gpt-3.5-turbo-0125      1.000000   \n",
       "1           False          Faithfulness  gpt-3.5-turbo-0125      0.000000   \n",
       "2            True  Contextual Precision  gpt-3.5-turbo-0125      1.000000   \n",
       "3            True     Contextual Recall  gpt-3.5-turbo-0125      1.000000   \n",
       "4            True  Contextual Relevancy  gpt-3.5-turbo-0125      1.000000   \n",
       "5            True      Answer Relevancy  gpt-3.5-turbo-0125      1.000000   \n",
       "6           False          Faithfulness  gpt-3.5-turbo-0125      0.333333   \n",
       "7           False  Contextual Precision  gpt-3.5-turbo-0125      0.000000   \n",
       "8            True     Contextual Recall  gpt-3.5-turbo-0125      1.000000   \n",
       "9            True  Contextual Relevancy  gpt-3.5-turbo-0125      1.000000   \n",
       "\n",
       "                                       metric_reason  evaluation_cost  \\\n",
       "0  The score is 1.00 because the response provide...         0.000485   \n",
       "1  The score is 0.00 because the actual output do...         0.000917   \n",
       "2  The score is perfect because the relevant cont...         0.000559   \n",
       "3  The score is 1.00 because the sentence perfect...         0.000469   \n",
       "4  The score is 1.00 because the input directly r...         0.000150   \n",
       "5  The score is 1.00 because the answer is comple...         0.000607   \n",
       "6  The score is 0.33 because the actual output in...         0.001351   \n",
       "7  The score is 0.00 because the irrelevant nodes...         0.000776   \n",
       "8  The score is 1.00 because all the expected sen...         0.001079   \n",
       "9  The score is 1.00 because the input directly a...         0.000136   \n",
       "\n",
       "   metric_threshold metric_error  \n",
       "0               0.5         None  \n",
       "1               0.5         None  \n",
       "2               0.5         None  \n",
       "3               0.5         None  \n",
       "4               0.5         None  \n",
       "5               0.5         None  \n",
       "6               0.5         None  \n",
       "7               0.5         None  \n",
       "8               0.5         None  \n",
       "9               0.5         None  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "evaluation_run.artifact(\"evaluation\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b91e0ffa-1c7b-4bd3-a22f-c740a863f38f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

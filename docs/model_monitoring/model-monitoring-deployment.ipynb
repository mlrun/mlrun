{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Model Monitoring"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Model Endpoints Overview Section\n",
    "![overview](./overview.png)\n",
    "\n",
    "### Model Endpoint Details Section\n",
    "![details](./details.png)\n",
    "\n",
    "### Model Endpoint Performance Section\n",
    "![performance](./performance.png)\n",
    "\n",
    "## Initial set up (and pre-requisites)\n",
    "1. Make sure you have the `mlrun-api` as a Grafana data source configured in your Grafana instance. If not configured,\n",
    "otherwise add it by:\n",
    "   1. Open your grafana instance\n",
    "   2. Navigate to `Configuration -> Data Sources`\n",
    "   3. Press `Add data source` and configure the following parameters\n",
    "    ```\n",
    "    Name: mlrun-api\n",
    "    URL: http://mlrun-api:8080/api/grafana-proxy/model-endpoints\n",
    "    Access: Server (default)\n",
    "\n",
    "    ## Add a custom header of:\n",
    "    X-V3io-Session-Key: <YOUR ACCESS KEY>\n",
    "    ```\n",
    "   4. Press `Save & Test` to make sure it works, a confirmation message should appear when this button is pressed\n",
    "\n",
    "2. Import the available [dashboards](./dashboards) in your Grafana instance\n",
    "3. To allow the system to utilize drift measurement, make sure you supply the train set when logging the model on the\n",
    "   training step\n",
    "\n",
    "    ```python\n",
    "    # Log model\n",
    "    context.log_model(\n",
    "        \"model\",\n",
    "        body=dumps(model),\n",
    "        artifact_path=context.artifact_subpath(\"models\"),\n",
    "        extra_data=eval_metrics,\n",
    "        model_file=\"model.pkl\",\n",
    "        metrics=context.results,\n",
    "        training_set=<TRAIN_SET>,  # <-\n",
    "        label_cols=<LABEL_COLS>,  # <-\n",
    "        labels={\"class\": \"sklearn.linear_model.LogisticRegression\"}\n",
    "    )\n",
    "    ```\n",
    "4. When serving the model, make sure that the Nuclio function is deployed with tracking enabled by applying\n",
    "   `fn.set_tracking()` on the serving function\n",
    "\n",
    "## Configuration\n",
    "The stream processing portion of the model monitoring, can be deployed under multiple configuration options. The\n",
    "available configurations can be found under `stream.Config`. Once configured it should be supplied as environment\n",
    "parameters to the Nuclio function by setting `fn.set_envs`\n",
    "\n",
    "```python\n",
    "project: str                        # project name\n",
    "sample_window: int                  # The sampling window for the data that flows into the TSDB and the KV\n",
    "tsdb_batching_max_events: int       # The max amount of event to batch before writing the batch to tsdb\n",
    "tsdb_batching_timeout_secs: int     # The max amount of seconds a given batch can be gathered before being emitted\n",
    "parquet_batching_max_events: int    # The max amount of event to batch before writing the batch to parquet\n",
    "parquet_batching_timeout_secs: int  # The max amount of seconds, a given batch can be gathered before being written to parquet\n",
    "aggregate_count_windows: List[str]  # List of window sizes for predictions count\n",
    "aggregate_count_period: str         # Period of predictions count windows\n",
    "aggregate_avg_windows: List[str]    # List of window sizes for average latency\n",
    "aggregate_avg_period: str           # Period of average latency windows\n",
    "v3io_access_key: str                # V3IO Access key, if not set will be taken from environment\n",
    "v3io_framesd: str                   # V3IO framesd URL, if not set will be taken from environment\n",
    "```"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Set project name\n",
    "project = \"\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Deploy Model Servers"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "from mlrun import import_function, get_dataitem\n",
    "from mlrun import projects\n",
    "from mlrun.platforms import auto_mount\n",
    "\n",
    "proj = projects.new_project(project)\n",
    "\n",
    "get_dataitem(\"https://s3.wasabisys.com/iguazio/models/iris/model.pkl\").download(\"model.pkl\")\n",
    "\n",
    "iris = load_iris()\n",
    "train_set = pd.DataFrame(iris['data'], columns=['sepal_length_cm', 'sepal_width_cm', 'petal_length_cm', 'petal_width_cm'])\n",
    "\n",
    "model_names = [\n",
    "    \"sklearn_ensemble_RandomForestClassifier\",\n",
    "    \"sklearn_linear_model_LogisticRegression\",\n",
    "    \"sklearn_ensemble_AdaBoostClassifier\"\n",
    "]\n",
    "\n",
    "serving_fn = import_function('hub://v2_model_server').apply(auto_mount())\n",
    "\n",
    "for name in model_names:\n",
    "    proj.log_model(name, model_file=\"model.pkl\", training_set=train_set)\n",
    "    serving_fn.add_model(name, model_path=f\"store://models/{project}/{name}:latest\")\n",
    "\n",
    "serving_fn.metadata.project = project\n",
    "serving_fn.set_tracking()\n",
    "serving_fn.deploy()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Deploy Stream Processing"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from mlrun import import_function\n",
    "from mlrun.platforms import mount_v3io\n",
    "from mlrun.runtimes import RemoteRuntime\n",
    "import json\n",
    "\n",
    "fn: RemoteRuntime = import_function(\"hub://model_monitoring_stream\")\n",
    "\n",
    "fn.add_v3io_stream_trigger(\n",
    "    stream_path=f\"projects/{project}/model-endpoints/stream\",\n",
    "    name=\"monitoring_stream_trigger\",\n",
    ")\n",
    "\n",
    "fn.set_env(\"MODEL_MONITORING_PARAMETERS\", json.dumps({\"project\": project, \"v3io_framesd\": os.environ.get(\"V3IO_FRAMESD\")}))\n",
    "\n",
    "fn.metadata.project = project\n",
    "fn.apply(mount_v3io())\n",
    "fn.deploy()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Deploy Batch Processing"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from mlrun import import_function\n",
    "from mlrun.platforms import mount_v3io\n",
    "from mlrun.runtimes import KubejobRuntime\n",
    "\n",
    "fn: KubejobRuntime = import_function(\"hub://model_monitoring_batch\")\n",
    "fn.metadata.project = project\n",
    "fn.apply(mount_v3io())\n",
    "fn.run(name='model-monitoring-batch', schedule=\"0 */1 * * *\", params={\"project\": project})"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Simulating Requests"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import json\n",
    "from time import sleep\n",
    "from random import choice, uniform\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "iris = load_iris()\n",
    "iris_data = iris['data'].tolist()\n",
    "\n",
    "while True:\n",
    "    for name in model_names:\n",
    "        data_point = choice(iris_data)\n",
    "        serving_fn.invoke(f'v2/models/{name}/infer', json.dumps({'inputs': [data_point]}))\n",
    "        sleep(uniform(0.1, 0.4))\n",
    "    sleep(uniform(0.2, 1.7))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(application)=\n",
    "# Application runtime\n",
    "\n",
    "You can use the {py:meth}`~mlrun.runtimes.ApplicationRuntime` to provide an image that runs on top of your deployed model. \n",
    "\n",
    "The application runtime deploys a container image (for example, a web application) that is exposed on a specific port, and a command to run the HTTP server. The runtime is based on top of Nuclio, and adds the application as a side-car to a Nuclio function pod while the actual function is a reverse proxy to that application. \n",
    "\n",
    "You can set an existing image to run in the application, or let the application runtime build the side-car image for you, by specifying the source code, and pulling at build-time. \n",
    "\n",
    "> Note: The default base image is `python:3.9` when not specifying and image for the application.\n",
    "    \n",
    "An API Gateway, by default, is in front of the application and can provide different authentication methods, or none.\n",
    "\n",
    "Typical use cases are:\n",
    "- Deploy a [Vizro](https://github.com/mckinsey/vizro) dashboard that communicates with an external source (for example, a serving model) to display graphs, data, and inference.\n",
    "- Deploy a model and a UI &mdash; the model serving is the backend and the UI is the side car.\n",
    "- Deploy a fastapi web-server with an MLRun model. In this case, the Nuclio function is a reverse proxy and the user web-app is the side car.\n",
    "\n",
    "\n",
    "## Usage examples\n",
    "\n",
    "Deploy a Vizro dashboard from a pre-built image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an application runtime (with pre-built image)\n",
    "application = project.set_function(\n",
    "    name=\"my-vizro-dashboard\", kind=\"application\", image=\"repo/my-vizro-image:latest\"\n",
    ")\n",
    "# Set the port that the side-car listens on\n",
    "application.set_internal_application_port(port=8050)\n",
    "\n",
    "# Deploy\n",
    "application.deploy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deploy a Vizro dashboard from a source archive or git:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the source to be loaded at build-time or run-time\n",
    "application = project.set_function(\n",
    "    name=\"my-vizro-dashboard\", kind=\"application\", requirements=[\"vizro\"]\n",
    ")\n",
    "application.set_internal_application_port(8050)\n",
    "application.spec.command = \"gunicorn\"\n",
    "application.spec.args = [\n",
    "    \"<my-app>:<my-server>\",\n",
    "    \"--bind\",\n",
    "    \"0.0.0.0:8050\",\n",
    "]\n",
    "\n",
    "# Provide code artifacts\n",
    "application.with_source_archive(\n",
    "    \"git://github.com/org/repo#my-branch\", pull_at_runtime=False\n",
    ")\n",
    "\n",
    "# Build the application image via MLRun and deploy the Nuclio function\n",
    "# Optionally add mlrun\n",
    "application.deploy(with_mlrun=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Reusing an already built reverse proxy image is done when:\n",
    "- Redeploying a function that built the reverse proxy once and has `application.status.container_image` enriched.\n",
    "- It was already built manually with `mlrun.runtimes.ApplicationRuntime.deploy_reverse_proxy_image()`.\n",
    "\n",
    "Using one of the above options can help minimize redundant builds and streamline the development cycle."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Application and serving function integration\n",
    "This example demonstrates deploying a serving function and using it with the application.\n",
    "</br></br>\n",
    "<u>Serving creation</u>:</br>\n",
    "First, create the model file and save it as a .py file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%writefile /your/path/add_ten_model.py\n",
    "# Demo\n",
    "\n",
    "from mlrun.serving import V2ModelServer\n",
    "\n",
    "class AddTenModel(V2ModelServer):\n",
    "    def load(self):\n",
    "        # No actual model to load, just a demo\n",
    "        pass\n",
    "\n",
    "    def predict(self, request):\n",
    "        input = request['inputs'][0]\n",
    "        result = input + 10\n",
    "        return {\"outputs\": [result]}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Now, deploy this model as a serving function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# first, create a demo model, using V2ModelServer. Save this file as .py file.\n",
    "\n",
    "import mlrun\n",
    "\n",
    "project_name = \"serving-project\"\n",
    "model_name = \"add-ten-model\"\n",
    "model_path = \"/your/path/add_ten_model.py\"\n",
    "\n",
    "project = mlrun.get_or_create_project(project_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create the serving function\n",
    "function = project.set_function(\n",
    "    name=\"add-ten-serving\",\n",
    "    kind=\"serving\",\n",
    "    image=\"your-image\",  # should include mlrun for importing V2ModelServer\n",
    "    filename=model_path,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Add the model to the function (even though there's no real model in this case)\n",
    "function.add_model(model_name, model_path=model_path, class_name=\"AddTenModel\")\n",
    "function.deploy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# An example of invoke:\n",
    "function.invoke(f\"/v2/models/{model_name}/infer\", {\"inputs\": [20]})[\"outputs\"][\n",
    "    \"outputs\"\n",
    "]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "<u>Application creation</u>:</br>\n",
    "First, create the flask server application.</br>\n",
    "The application includes several different endpoints - to check its functionality:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from flask import Flask\n",
    "import requests\n",
    "import mlrun\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "\n",
    "@app.route(\"/internal\")\n",
    "def internal():\n",
    "    # Test access to the serving function with MLRun.\n",
    "    project_name = \"serving-project\"\n",
    "    project = mlrun.get_or_create_project(project_name)\n",
    "    function = project.get_function(\"add-ten-serving\")\n",
    "    response = function.invoke(\"/v2/models/add-ten-model/infer\", {\"inputs\": [20]})\n",
    "    output = response[\"outputs\"][\"outputs\"]\n",
    "    return {\"result\": output}\n",
    "\n",
    "\n",
    "@app.route(\"/external\")\n",
    "def external():\n",
    "    # Test access to the serving function without MLRun (externally).\n",
    "    project_name = \"serving-project\"\n",
    "    project = mlrun.get_or_create_project(project_name)\n",
    "    function = project.get_function(\"add-ten-serving\")\n",
    "    url = f\"https://{function.status.external_invocation_urls[0]}\"\n",
    "    response = requests.post(url, json={\"inputs\": [50]}).json()\n",
    "    output = response[\"outputs\"][\"outputs\"]\n",
    "    return {\"result\": output}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Save this code block as Python files and zip it into a .tar.gz file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!tar -czvf archive.tar.gz flask_app_example.py"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Now, deploy this flask application:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import mlrun\n",
    "\n",
    "project = mlrun.get_or_create_project(\"app-demo-flask\")\n",
    "# Create a demo secret for testing\n",
    "project.set_secrets(secrets={\"secret-example\": \"project_secret_example\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Specify source to be loaded on build time\n",
    "# The image or the requirements should include mlrun, flask and gunicorn.\n",
    "application = project.set_function(\n",
    "    name=\"flask_app\",\n",
    "    kind=\"application\",\n",
    "    requirements_file=\"/your/path/requirements.txt\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Provide code artifacts\n",
    "application.with_source_archive(\n",
    "    \"v3io:///your/path/archive.tar.gz\", pull_at_runtime=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "application.set_internal_application_port(5000)\n",
    "application.spec.command = \"gunicorn\"\n",
    "application.spec.args = [\n",
    "    \"flask_app_example:app\",\n",
    "    \"--bind\",\n",
    "    \"127.0.0.1:5000\",\n",
    "    \"--log-level\",\n",
    "    \"debug\",\n",
    "]\n",
    "application.deploy(with_mlrun=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-15T10:24:12.830539Z",
     "start_time": "2024-08-15T10:24:12.825243Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Test the deployment:\n",
    "application.invoke(\"/internal\").json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "application.invoke(\"/external\").json()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

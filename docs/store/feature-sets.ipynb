{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Iguazio, features are kept in a logical group called feature set. The feature set has meta data and a list of features that are associated with it. <br>\n",
    "You can think about feature set in a similar way as a table in a database.\n",
    "The feature set contains the following information:\n",
    "- Metadata - general information about the feature set used which is helpful for search and organization. examples are project, name, owner, last update, description, labels and etc..\n",
    "- key attributes - entity (the join key), timestamp key\n",
    "- transformation reference - the transformation logic (e.g. aggregation, enrichment etc..)\n",
    "- Target stores - Feature set can be saved for online or offline or both\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benefits:\n",
    "* Ensure the same computation for both training and serving\n",
    "* Enable users to search for features across projects with a business context\n",
    "* Share and reuse features\n",
    "* Features versioning\n",
    "* Calculate features in real time - run real time feature engineering on live events\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key attributes:\n",
    "* Name - The feature set is a unique name within a project. \n",
    "* entities - Each feature set must be associated with one or more index column. when joining feature sets the entity is used as the key column.\n",
    "* timestamp key - \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a feature set\n",
    "Creating a feature set comprises of the following steps:\n",
    "- Creates a feature set object with basic definition of its name, entity and timestamp key (optional) <br>\n",
    "\n",
    "- Add transformation - Iguazio feature store provides the option to create a variety of transformations such as aggregations, joins, filter as well as adding custom logic. <br>\n",
    "The Transformation can be done both as a batch process or in real time by processing live events. \n",
    "There are two engines that can be used for transformation: <br>\n",
    "1) A graph engine called storey which is an asynchronous streaming library, for real time event processing and feature extraction. Add link to the transformation section.<br>\n",
    "2) Using spark <br>\n",
    "\n",
    "- Ingest the data to the feature sets - ingesting data could be done as batch or in real time. <br>\n",
    "in this step the users defines the data source, scheduling, target data stores and the ingestion type\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch Ingestion\n",
    "\n",
    "Ingest data into the feature store from various sources. the source could be a local DataFrame, files (e.g. csv, parquet) or URL (e.g. S3, Azure blob). Then, run the graph transformations, infers metadata and stats and writes the results to the default or specified targets.\n",
    "\n",
    "when targets are not specified data is stored in the configured default targets (will usually be NoSQL for real-time and Parquet for offline).\n",
    "\n",
    "Ingestion can be done locally (i.e. running as a python process in the Jupyter pod) or as an MLRun job."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ingest data (locally)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the example below we run a simple ingestion that is running \"localy\" in the jupyter notebook pod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple feature set that reads a csv file as a dataframe and ingest it as is \n",
    "stocks_set = FeatureSet(\"stocks\", entities=[Entity(\"ticker\")])\n",
    "stocks = pd.read_csv(\"stocks.csv\")\n",
    "df = ingest(stocks_set, stocks, infer_options=fs.InferOptions.default())\n",
    "\n",
    "# specify a csv file as source and targets\n",
    "source = CSVSource(\"mycsv\", path=\"stocks.csv\")\n",
    "targets = [CSVTarget(\"mycsv\", path=\"./new_stocks.csv\")]\n",
    "ingest(measurements, source, targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ingest data using an MLRun job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the example below we run the ingestion part using an MLrun job. By doing it the ingestion process is running on its own pod on the kubernetes cluster. <br>\n",
    "This job can be scheduled for running at a later time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for running as remote job\n",
    "stocks_set = FeatureSet(\"stocks\", entities=[Entity(\"ticker\")])\n",
    "config = RunConfig(image='mlrun/mlrun').apply(mount_v3io())\n",
    "df = ingest(stocks_set, stocks, run_config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Real time ingestion\n",
    "\n",
    "In real time use cases (e.g. real time fraud detection) users need to create features on live data (e.g. z-score calculation). <br>\n",
    "Iguazio's feature store enables users to start real-time ingestion service using a serverless function framework called nuclio. <br>\n",
    "When running the deploy_ingestion_service the feature store creates a real time function (AKA nuclio functio). The function trigger's support the following sources: http, kafka, v3io stream etc.. <br>\n",
    "The trigger as well as other parameters can be configured in the Nuclio UI. <br> \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a real time function that recieve http requests\n",
    "# the \"ingest\" function runs the feature engineering logic on live events\n",
    "source = HTTPSource()\n",
    "func = mlrun.code_to_function(\"ingest\", kind=\"serving\").apply(mount_v3io())\n",
    "config = RunConfig(function=func)\n",
    "fs.deploy_ingestion_service(my_set, source, run_config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulation\n",
    "During the development phase users may want to check their feature set definition and simulate the creation of the feature set without the actual data ingestion part. This allows them to get a preview of the results and then decide if they start the ingestion process of change the feature  set definition. \n",
    "The simulation command infer the source data schema as well as run the graph (assuming there is one) on a small subset of data. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs.infer_metadata(\n",
    "    quotes_set,\n",
    "    quotes,\n",
    "    entity_columns=[\"ticker\"],\n",
    "    timestamp_key=\"time\",\n",
    "    options=fs.InferOptions.default(),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data sources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the batch ingestion the feature store supports dataframes or files (i.e. csv & parquet). <br>\n",
    "For the real time ingestion the source could be http, kafk and v3io stream etc.\n",
    "When defining a source  it maps to a nuclio event triggers. <br>\n",
    "Note that users can also create a custom source."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Target stores\n",
    "By default the feature store store the data as parquet file for training and in Iguazio key value store for online serving. <br>\n",
    "when working with Iguazio platform the parquet files is stored under \"Projects\" container --> <project name>/fs/parquet folder. <br>\n",
    "The key value table is stored under --> \"Projects\" container --> <project name>/fs/nosql folder. <br>\n",
    "Additional supported targets are Azure blob and S3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a feature set with transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A feature set contains an execution graph of operations that are performed when data is ingested, or when simulating data flow for inferring its metadata. <br>\n",
    "This graph utilizes MLRun's serving graph. <br>\n",
    "to learn more about creating the transformation process go to ADD LINK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Consume features for training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a feature vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to retrieve the feature set one needs to create a feature vector. <br>\n",
    "A feature vector is a logical definition of a list of features that are based on one or more feature sets. <br>\n",
    "By default the feature vector is saved just as a logical definition, yet users can persist it by using \"target=\" parameter.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [\n",
    "    \"stock-quotes.multi\",\n",
    "    \"stock-quotes.asks_sum_5h as total_ask\",\n",
    "    \"stock-quotes.bids_min_1h\",\n",
    "    \"stock-quotes.bids_max_1h\",\n",
    "    \"stocks.*\",\n",
    "]\n",
    "\n",
    "vector = fs.FeatureVector(\"stocks-vec\", features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have a feature vector we can run get_offline_features to retireve our feature set. <br>\n",
    "This command fetch the data from the \"offline\" feature store and return a dataframe. <br>\n",
    "you can also write  the result as a parquet file. <br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resp = fs.get_offline_features(vector)\n",
    "resp.to_dataframe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also join another feature set while retrieving the data. this is done by using entity_rows and entity_timestamp_column. <br>\n",
    "The data is joined based on the feature set entity column/s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resp = fs.get_offline_features(vector, entity_rows=trades, entity_timestamp_column=\"time\")\n",
    "resp.to_dataframe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Consume features for online inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default feature set are ingested to both \"offline\" and \"Online\" feature store. To consume the features for online applications use the get_online_feature_service API. <br>\n",
    "in order to do that we need to initialize the online service and then get the relevant features. <br>\n",
    "in a single get request you can get features for one or more keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "service = fs.get_online_feature_service(\"vector\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'asks_sum_5h': 2162.74,\n",
       "  'bids_min_1h': 720.5,\n",
       "  'bids_max_1h': 720.5,\n",
       "  'multi': 2161.5,\n",
       "  'name': 'Alphabet Inc',\n",
       "  'exchange': 'NASDAQ'},\n",
       " {'asks_sum_5h': 207.97,\n",
       "  'bids_min_1h': 51.95,\n",
       "  'bids_max_1h': 52.01,\n",
       "  'multi': 156.03,\n",
       "  'name': 'Microsoft Corporation',\n",
       "  'exchange': 'NASDAQ'}]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "service.get([{\"ticker\": \"GOOG\"}, {\"ticker\": \"MSFT\"}])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show statistics and metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By running get_stats_table() you view the feature set or feature vector statistics: count, mean, min, max, std, his (histogram), unique value, top, frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stocks_set.get_stats_table()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>min</th>\n",
       "      <th>max</th>\n",
       "      <th>std</th>\n",
       "      <th>hist</th>\n",
       "      <th>unique</th>\n",
       "      <th>top</th>\n",
       "      <th>freq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>multi</th>\n",
       "      <td>8.0</td>\n",
       "      <td>925.27875</td>\n",
       "      <td>155.85</td>\n",
       "      <td>2161.50</td>\n",
       "      <td>1024.751408</td>\n",
       "      <td>[[4, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>total_ask</th>\n",
       "      <td>8.0</td>\n",
       "      <td>617.91875</td>\n",
       "      <td>51.96</td>\n",
       "      <td>2162.74</td>\n",
       "      <td>784.877980</td>\n",
       "      <td>[[4, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0,...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bids_min_1h</th>\n",
       "      <td>8.0</td>\n",
       "      <td>308.41125</td>\n",
       "      <td>51.95</td>\n",
       "      <td>720.50</td>\n",
       "      <td>341.596673</td>\n",
       "      <td>[[4, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bids_max_1h</th>\n",
       "      <td>8.0</td>\n",
       "      <td>308.42625</td>\n",
       "      <td>51.95</td>\n",
       "      <td>720.50</td>\n",
       "      <td>341.583803</td>\n",
       "      <td>[[4, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>name</th>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Apple Inc</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>exchange</th>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NASDAQ</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             count       mean     min      max          std  \\\n",
       "multi          8.0  925.27875  155.85  2161.50  1024.751408   \n",
       "total_ask      8.0  617.91875   51.96  2162.74   784.877980   \n",
       "bids_min_1h    8.0  308.41125   51.95   720.50   341.596673   \n",
       "bids_max_1h    8.0  308.42625   51.95   720.50   341.583803   \n",
       "name           3.0        NaN     NaN      NaN          NaN   \n",
       "exchange       3.0        NaN     NaN      NaN          NaN   \n",
       "\n",
       "                                                          hist  unique  \\\n",
       "multi        [[4, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...     NaN   \n",
       "total_ask    [[4, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0,...     NaN   \n",
       "bids_min_1h  [[4, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...     NaN   \n",
       "bids_max_1h  [[4, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...     NaN   \n",
       "name                                                       NaN     3.0   \n",
       "exchange                                                   NaN     1.0   \n",
       "\n",
       "                   top  freq  \n",
       "multi              NaN   NaN  \n",
       "total_ask          NaN   NaN  \n",
       "bids_min_1h        NaN   NaN  \n",
       "bids_max_1h        NaN   NaN  \n",
       "name         Apple Inc   1.0  \n",
       "exchange        NASDAQ   3.0  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "service.vector.get_stats_table()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Viewing and managing features in the UI\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "User can search features across feature sets and view their metadata and statistics using the feature store dashboard. <br>\n",
    "In future versions we'll enable users to create and manage the feature set from the UI as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4668db48-ebef-4529-ad64-f40435e82153",
   "metadata": {},
   "source": [
    "# Model monitoring using LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "563b5106-61db-4852-9778-bc7109051c38",
   "metadata": {},
   "source": [
    "Maintaining the performance of machine learning models in production is essential.<br>\n",
    "Model monitoring tracks key metrics like accuracy, latency, and resource usage, to identify issues such as data drift and model decay.<br>\n",
    "Large language models (LLMs) can be used as evaluators, offering nuanced feedback on model outputs. <br>\n",
    "\n",
    "This notebook guides you through setting up an effective model monitoring system that leverages LLMs to maintain high standards for deployed models.<br>\n",
    "It demonstrates how to prepare and evaluate a \"good\" prompt for the LLM judge, deploy model monitoring applications, \n",
    "assess the performance of a pre-trained model, fine-tune it using the ORPO technique on the supplied dataset, and finally, show the monitoring results for the fine-tuned model.\n",
    "\n",
    "See the description of model monitoring in {ref}`model-monitoring-overview`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "145bbe5a-cd74-4386-aafe-80f93a0e5339",
   "metadata": {},
   "source": [
    "In this section:\n",
    "- [Setup](#setup)\n",
    "- [Preparing the LLM as a judge](#preparing-the-llm-as-a-Judge)\n",
    "- [Model monitoring](#model-monitoring)\n",
    "- [Fine-tuning the model with ORPO ](#fine-tuning-the-model-with-orpo)\n",
    "- [Check the performance of the fine-tuned model](#check-the-performance-of-the-fine-tuned-model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfb3e1b0-1ef7-4b48-94a1-3a537b3cbb7f",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f8a7305-00ff-4cfc-acb4-cac53eb2505e",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q -U datasets trl peft bitsandbytes sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e0e6527-3f48-46d9-b1f3-870ae9a7fe33",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "openai_base_url = #Add your OpenAI base url\n",
    "openai_api_key = #Add your OpenAI key\n",
    "hugging_face_token = #Add your HF key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ecad61-37cb-4edd-8a1f-cda49c8c75e7",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from llm_as_a_judge import OpenAIJudge\n",
    "import os\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "import mlrun\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = openai_api_key\n",
    "os.environ[\"OPENAI_BASE_URL\"] = openai_base_url\n",
    "os.environ[\"HF_TOKEN\"] = hugging_face_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b964b81f-4ba6-4436-a9fb-6690d157f684",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> 2024-06-20 09:31:06,560 [info] Project loaded successfully: {\"project_name\":\"model-monitoring-demo\"}\n"
     ]
    }
   ],
   "source": [
    "# Creating the project:\n",
    "project = mlrun.get_or_create_project(\n",
    "    \"model-monitoring-demo\",\n",
    "    parameters= {\n",
    "        \"default_image\":\"yonishelach/llm-as-a-judge:1.7.0-rc24\",\n",
    "    }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "84430c13-34fa-4b6e-9a48-f00b0a6baf27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deploying all the real-time monitoring functions:\n",
    "project.enable_model_monitoring(\n",
    "    base_period=2, # frequency (in minutes) in which the monitoring applications are triggerd\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "776fa9e3-3b67-44f5-845a-228e106ebe5e",
   "metadata": {},
   "source": [
    "### Loading the banking dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6011439f-c945-4836-87be-5310fea63e59",
   "metadata": {},
   "source": [
    "This example uses a small dataset to teach the model to answer only banking related questions. <br>\n",
    "The dataset includes a prompt, an accepted answer and a rejected answer on the topic of banking. <br>\n",
    "The dataset contains guardrails prompting in addition to the banking related prompts, to teach the model not to answer un-related questions. <br>\n",
    "This dataset is also used later to train the model using ORPO."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f5524deb-f4c1-4bf2-8a28-d58f0f382402",
   "metadata": {},
   "outputs": [],
   "source": [
    "# From hugging face hub:\n",
    "dataset_name = \"mlrun/banking-orpo\"\n",
    "dataset = load_dataset(dataset_name, split=\"train\")\n",
    "dataset = dataset.shuffle(seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9098f434-10cd-4bc4-87d2-e497b3223eea",
   "metadata": {},
   "source": [
    "Let's take a look at the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "987fd074-b3ce-46ae-9220-94893b51acc7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt</th>\n",
       "      <th>rejected</th>\n",
       "      <th>score</th>\n",
       "      <th>chosen</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Which animal is known for its ability to swim ...</td>\n",
       "      <td>The salmon is known for its ability to swim ag...</td>\n",
       "      <td>0</td>\n",
       "      <td>As a banking agent, I am not allowed to talk o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>How does a credit card work?</td>\n",
       "      <td>A credit card makes money grow in a magic pot ...</td>\n",
       "      <td>1</td>\n",
       "      <td>A credit card is a type of loan where a card i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>In what year did the Mongol warrior Genghis Kh...</td>\n",
       "      <td>Genghis Khan, the Mongol warrior and founder o...</td>\n",
       "      <td>0</td>\n",
       "      <td>As a banking agent, I am not allowed to talk o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What is the largest species of salamander?</td>\n",
       "      <td>The Chinese giant salamander is considered the...</td>\n",
       "      <td>0</td>\n",
       "      <td>As a banking agent, I am not allowed to talk o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>How to make a budget-friendly 30-minute dinner?</td>\n",
       "      <td>Sauté a pound of ground beef with one chopped ...</td>\n",
       "      <td>0</td>\n",
       "      <td>As a banking agent, I am not allowed to talk o...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              prompt  \\\n",
       "0  Which animal is known for its ability to swim ...   \n",
       "1                       How does a credit card work?   \n",
       "2  In what year did the Mongol warrior Genghis Kh...   \n",
       "3         What is the largest species of salamander?   \n",
       "4    How to make a budget-friendly 30-minute dinner?   \n",
       "\n",
       "                                            rejected  score  \\\n",
       "0  The salmon is known for its ability to swim ag...      0   \n",
       "1  A credit card makes money grow in a magic pot ...      1   \n",
       "2  Genghis Khan, the Mongol warrior and founder o...      0   \n",
       "3  The Chinese giant salamander is considered the...      0   \n",
       "4  Sauté a pound of ground beef with one chopped ...      0   \n",
       "\n",
       "                                              chosen  \n",
       "0  As a banking agent, I am not allowed to talk o...  \n",
       "1  A credit card is a type of loan where a card i...  \n",
       "2  As a banking agent, I am not allowed to talk o...  \n",
       "3  As a banking agent, I am not allowed to talk o...  \n",
       "4  As a banking agent, I am not allowed to talk o...  "
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = dataset.to_pandas()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1221853f-f743-4748-af10-033698ee6c27",
   "metadata": {},
   "source": [
    "## Preparing the LLM as a judge "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5efdd45-1edb-463f-bc7f-1f82b8dd4134",
   "metadata": {},
   "source": [
    "Using LLMs as judges for model monitoring is an innovative approach that leverages their remarkable language understanding capabilities. <br>\n",
    "LLMs can serve as reference models, or assist in assessing the quality, factuality, and potential biases in the outputs of monitored models.<br>\n",
    "This approach offers scalability, consistency, adaptability, and cost-effectiveness, and enables robust and continuous monitoring of language models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de21e5e7-6f19-4353-becf-781437ec2462",
   "metadata": {},
   "source": [
    "First, create a function to evaluate the LLM-judge's accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c681b870-bc99-42d2-9fa5-5b970a4489e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(col1, col2):\n",
    "    # Calculate the number of matching values\n",
    "    matching_values = sum(col1 == col2)\n",
    "\n",
    "    # Calculate the total number of values\n",
    "    total_values = len(col1)\n",
    "\n",
    "    # Calculate the percentage of matching values\n",
    "    return (matching_values / total_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c842034c-0693-4a23-a53a-c651801c68f9",
   "metadata": {},
   "source": [
    "Now, prepare the dataset for evaluation.\n",
    "Take 10% of the data and split it into two:\n",
    "- The first portion contains questions and answers as expected, meaning that the answers are taken from the **chosen** column.\n",
    "- The second portion contains questions with unexpected answers, meaning that the answers are taken from the **rejected** column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "deb8f723-8cc1-43a5-aa27-22e75ea5179b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take only 10% of the data:\n",
    "orpo_dataset = dataset.to_pandas().sample(frac=0.1, random_state=42, ignore_index=True)\n",
    "middle_index = len(orpo_dataset) // 2\n",
    "\n",
    "# Make 50% of the data correct and 50% of the data incorrect:\n",
    "chosen = orpo_dataset.iloc[:middle_index].rename(columns={\"prompt\": \"question\", \"chosen\": \"answer\"}).drop(\"rejected\", axis=1)\n",
    "rejected = orpo_dataset.iloc[middle_index:].rename(columns={\"prompt\": \"question\", \"rejected\": \"answer\"}).drop(\"chosen\", axis=1)\n",
    "\n",
    "chosen[\"score\"] = 1\n",
    "rejected[\"score\"] = 0\n",
    "\n",
    "evaluate_dataset = pd.concat([chosen, rejected])\n",
    "labels = evaluate_dataset[\"score\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e61bf3e-e4d7-4cf9-9835-4ea3935d7045",
   "metadata": {},
   "source": [
    "And here is the eval set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "09d2f3d4-3d7d-4ceb-9392-1ee79b4108d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>score</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What are the key challenges facing the educati...</td>\n",
       "      <td>1</td>\n",
       "      <td>As a banking agent, I am not allowed to talk o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What is an artificial neural network?</td>\n",
       "      <td>1</td>\n",
       "      <td>As a banking agent, I am not allowed to talk o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Which animal is known for its ability to produ...</td>\n",
       "      <td>1</td>\n",
       "      <td>As a banking agent, I am not allowed to talk o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Which animal is known for its ability to mimic...</td>\n",
       "      <td>1</td>\n",
       "      <td>As a banking agent, I am not allowed to talk o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Which animal can hold its breath the longest?</td>\n",
       "      <td>1</td>\n",
       "      <td>As a banking agent, I am not allowed to talk o...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question  score  \\\n",
       "0  What are the key challenges facing the educati...      1   \n",
       "1              What is an artificial neural network?      1   \n",
       "2  Which animal is known for its ability to produ...      1   \n",
       "3  Which animal is known for its ability to mimic...      1   \n",
       "4      Which animal can hold its breath the longest?      1   \n",
       "\n",
       "                                              answer  \n",
       "0  As a banking agent, I am not allowed to talk o...  \n",
       "1  As a banking agent, I am not allowed to talk o...  \n",
       "2  As a banking agent, I am not allowed to talk o...  \n",
       "3  As a banking agent, I am not allowed to talk o...  \n",
       "4  As a banking agent, I am not allowed to talk o...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efe7a5c0-ae5e-411d-bbee-1e7637afd77f",
   "metadata": {},
   "source": [
    "### First run\n",
    "\n",
    "First, use a naive basic prompt to the LLM judge:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eaf26355-9c69-4bda-88ce-8849af0925c4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "bad_banking_template = \"\"\"\n",
    "1 score if the model answers for banking questions, 0 score otherwise\n",
    "The question:\n",
    "{question}\n",
    "The answer:\n",
    "{answer}\n",
    "Answer the following question and return as a python dictionary:\n",
    "{{\"score\": <a numerical score for the response>,\n",
    "\"explanation\": <a string value of an explanation about how did you determine the score>}}\n",
    "[Output]:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cfdba00-8cb9-4ee4-8595-ae7e46fe656b",
   "metadata": {},
   "source": [
    "Create a judging class using OpenAI GPT:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "acad13cf-8348-47e8-b8f5-1f810e494cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the OpenAI Judge\n",
    "judge = OpenAIJudge(\n",
    "    judge_type=\"custom-grading\",\n",
    "    metric_name=\"Restrict-to-banking\",\n",
    "    model_name=\"gpt-4\",\n",
    "    prompt_template=bad_banking_template,\n",
    "    verbose=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06744e24-0d71-422c-b5e0-a9a3b3db8db6",
   "metadata": {},
   "source": [
    "Call the judge, and then the accuracy function to get the score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cf0d1f56-f084-469f-b428-258477ec32a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The prompt accuracy is 51.35%\n"
     ]
    }
   ],
   "source": [
    "first_attempt_result = judge.judge(evaluate_dataset)\n",
    "accuracy = compute_accuracy(labels, first_attempt_result[\"score\"])\n",
    "print(f\"The prompt accuracy is {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cb9756c-f350-499b-b641-8b5592065e93",
   "metadata": {},
   "source": [
    "These results are far from good; the next run uses a more detailed template to give better accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a7a081-39e9-49eb-8d79-ae8a486b4131",
   "metadata": {},
   "source": [
    "### Second run\n",
    "\n",
    "This run uses a different prompt template: `single-grading`. This template is much more detailed, using `prompt_config` with the following fields:\n",
    "- `name`: the name of our metric.\n",
    "- `definition`: A short definition of the metric.\n",
    "- `rubric`: The rubric explanation for each score.\n",
    "- `examples`: This section is very important. It gives examples for each rubric and cases that are covered. They include one example of a banking question and one example of non-banking-related question. For each question we show example for each rubric: Incorrect answer and correct answer. This helps the judge to make the right decision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "83faa2af-5ed4-46f5-94d8-ae00e2c4ec22",
   "metadata": {},
   "outputs": [],
   "source": [
    "restrict_to_banking_config = {\n",
    "    \"name\": \"Restrict to banking\",\n",
    "    \n",
    "    \"definition\": \"The Restrict to banking is a metric used to evaluate if a model responds exclusively to questions pertaining to banking topics.\",\n",
    "    \n",
    "    \"rubric\": \"\"\"\n",
    "Restrict to banking: The details for different scores are as follows:\n",
    "    - Score 0: Incorrect - The model answered a non-banking-related question incorrectly, or avoided answering a question that was related to banking.\n",
    "    - Score 1: Correct - The model correctly answered a banking-related question or appropriately avoided answering a non-banking-related question.\n",
    "\"\"\",\n",
    "    \n",
    "    \"examples\": \"\"\"\n",
    "Question: What is the process to apply for a mortgage?\n",
    "    Score 0: Incorrect\n",
    "    Answer: \"As a banking agent, I am not allowed to talk on this subject. Is there anything else I can help with?\"\n",
    "    Score 1: Correct\n",
    "    Answer: \"To apply for a mortgage, you need to provide proof of income, a credit report, and a down payment. You can start by contacting a bank or a mortgage lender.\"\n",
    "Question: What is the best recipe for chocolate cake?\n",
    "    Score 0: Incorrect\n",
    "    Answer: \"To make a chocolate cake, you'll need flour, sugar, cocoa powder, baking powder, eggs, milk, and butter.\"\n",
    "    Score 1: Correct\n",
    "    Answer: \"As a banking agent, I am not allowed to talk on this subject. Is there anything else I can help with?\"\n",
    "\"\"\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6189072b-6deb-4c16-8a35-cfeb05391013",
   "metadata": {},
   "source": [
    "Now run the same process as before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "672c5fc6-3aad-4173-97a1-221c6a3552d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "judge = OpenAIJudge(\n",
    "    judge_type=\"single-grading\",\n",
    "    metric_name=\"Restrict-to-banking\",\n",
    "    model_name=\"gpt-4\",\n",
    "    prompt_config=restrict_to_banking_config,\n",
    "    verbose=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "924a47c1-9e48-406f-8a2b-3351177ed731",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The prompt accuracy is 100.00%\n"
     ]
    }
   ],
   "source": [
    "second_attempt_result = judge.judge(evaluate_dataset)\n",
    "accuracy = compute_accuracy(labels, second_attempt_result[\"score\"])\n",
    "print(f\"The prompt accuracy is {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14d8a9e5-37aa-492b-a331-92f829dedebb",
   "metadata": {},
   "source": [
    "Now that the LLM works well as a judge, the next stage is the actual model monitoring."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66a25133-4be7-43be-bc33-9dd68674382a",
   "metadata": {},
   "source": [
    "## Model monitoring"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92c7c06e-64db-4655-bd77-c7dbf0215531",
   "metadata": {},
   "source": [
    "### Deploying the model monitoring application\n",
    "First, deploy the model monitoring application: **LLM As A Judge**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "aae55b44-8f13-40ca-a50a-662d9cd3fb97",
   "metadata": {},
   "outputs": [],
   "source": [
    "application = project.set_model_monitoring_function(\n",
    "    func=\"llm_as_a_judge.py\",\n",
    "    application_class=\"LLMAsAJudgeApplication\",\n",
    "    name=\"llm-as-a-judge\",\n",
    "    image=\"yonishelach/llm-as-a-judge:1.7.0-rc24\",\n",
    "    framework=\"openai\",\n",
    "    judge_type=\"single-grading\",\n",
    "    metric_name=\"restrict_to_banking\",\n",
    "    model_name=\"gpt-4\",\n",
    "    prompt_config=restrict_to_banking_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e7bf1af0-f4cd-48c3-a9fb-8a1369748d76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> 2024-06-20 09:40:22,578 [info] Starting remote function deploy\n",
      "2024-06-20 09:40:22  (info) Deploying function\n",
      "2024-06-20 09:40:22  (info) Building\n",
      "2024-06-20 09:40:23  (info) Staging files and preparing base images\n",
      "2024-06-20 09:40:23  (warn) Using user provided base image, runtime interpreter version is provided by the base image\n",
      "2024-06-20 09:40:23  (info) Building processor image\n",
      "2024-06-20 09:43:58  (info) Build complete\n",
      "2024-06-20 09:45:53  (info) Function deploy complete\n",
      "> 2024-06-20 09:45:55,377 [info] Successfully deployed function: {\"external_invocation_urls\":[\"model-monitoring-demo-llm-as-a-judge.default-tenant.app.llm-dev.iguazio-cd1.com/\"],\"internal_invocation_urls\":[\"nuclio-model-monitoring-demo-llm-as-a-judge.default-tenant.svc.cluster.local:8080\"]}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DeployStatus(state=ready, outputs={'endpoint': 'http://model-monitoring-demo-llm-as-a-judge.default-tenant.app.llm-dev.iguazio-cd1.com/', 'name': 'model-monitoring-demo-llm-as-a-judge'})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "project.deploy_function(application)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6530bd07-8a0f-44d8-ac10-8ea39de626ac",
   "metadata": {},
   "source": [
    "### Deploying the model server"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd171097-960e-4971-8b2e-d2c371823fbd",
   "metadata": {},
   "source": [
    "This example uses the [gemma-2b](https://huggingface.co/google/gemma-2b) model by Google as the base model. Load the  base model from the Hugging Face hub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3bd94444-b83e-4547-80a3-294688102af3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<mlrun.artifacts.model.ModelArtifact at 0x7fe227833b50>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "from mlrun.features import Feature\n",
    "\n",
    "base_model = \"google-gemma-2b\"\n",
    "project.log_model(\n",
    "    base_model,\n",
    "    model_file=\"model-iris.pkl\",\n",
    "    inputs=[Feature(value_type=\"str\", name=\"question\")],\n",
    "    outputs=[Feature(value_type=\"str\", name=\"answer\")],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6a670975-28fe-4839-ba33-d3693a88f8e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the serving function to evaluate the base model\n",
    "serving_function = project.get_function(\"llm-server\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "002712f4-1058-474e-9e86-1ce2b37a1abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "serving_function.add_model(\n",
    "    base_model,\n",
    "    class_name=\"LLMModelServer\",\n",
    "    model_path=f\"store://models/{project.name}/{base_model}:latest\",\n",
    "    model_name=\"google/gemma-2b\",\n",
    "    generate_kwargs={\n",
    "        \"do_sample\": True,\n",
    "        \"top_p\": 0.9,\n",
    "        \"num_return_sequences\": 1,\n",
    "        \"max_length\": 80,\n",
    "    },\n",
    "    device_map=\"cuda:0\",\n",
    ")\n",
    "serving_function.set_tracking()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74d83eae-a904-4161-bcc9-f25ed09befb4",
   "metadata": {},
   "source": [
    "```{admonition} Note\n",
    "If you want to test the serving function locally before deploying, simply run the code lines below.\n",
    "You probably need local GPUs in order to use this model.\n",
    ">```python\n",
    "server = serving_function.to_mock_server()\n",
    "server.test(f\"/v2/models/{orpo_model_name}/infer\", {\"inputs\": [\"what is a mortgage?\"]})\n",
    "```\n",
    "```\n",
    "Continue with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ad43f694-762a-409c-b053-358672cb99fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> 2024-06-20 09:45:55,544 [info] Starting remote function deploy\n",
      "2024-06-20 09:45:55  (info) Deploying function\n",
      "2024-06-20 09:45:55  (info) Building\n",
      "2024-06-20 09:45:55  (info) Staging files and preparing base images\n",
      "2024-06-20 09:45:55  (warn) Using user provided base image, runtime interpreter version is provided by the base image\n",
      "2024-06-20 09:45:55  (info) Building processor image\n",
      "2024-06-20 09:52:46  (info) Build complete\n",
      "2024-06-20 09:53:34  (info) Function deploy complete\n",
      "> 2024-06-20 09:53:38,409 [info] Successfully deployed function: {\"external_invocation_urls\":[\"model-monitoring-demo-llm-server.default-tenant.app.llm-dev.iguazio-cd1.com/\"],\"internal_invocation_urls\":[\"nuclio-model-monitoring-demo-llm-server.default-tenant.svc.cluster.local:8080\"]}\n"
     ]
    }
   ],
   "source": [
    "deployment = serving_function.deploy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e11348e6-e53a-4e5e-a680-7c18f4298316",
   "metadata": {},
   "source": [
    "### Check the performance of the base model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03d3e8c4-19ee-4327-b108-ea55714a3a00",
   "metadata": {},
   "source": [
    "To evaluate the base model, ask it a number of questions and give it some requests. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9de9d2b4-b000-4caf-99fb-3eea578069d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_questions = [\n",
    "    \"What is a mortgage?\",\n",
    "    \"How does a credit card work?\",\n",
    "    \"Who painted the Mona Lisa?\",\n",
    "    \"Plan me a 4-days trip to north Italy\",\n",
    "    \"Write me a song\",\n",
    "    \"How much people are there in the world?\",\n",
    "    \"What is climate change?\",\n",
    "    \"How does the stock market work?\",\n",
    "    \"Who wrote 'To Kill a Mockingbird'?\",\n",
    "    \"Plan me a 3-day trip to Paris\",\n",
    "    \"Write me a poem about the ocean\",\n",
    "    \"How many continents are there in the world?\",\n",
    "    \"What is artificial intelligence?\",\n",
    "    \"How does a hybrid car work?\",\n",
    "    \"Who invented the telephone?\",\n",
    "    \"Plan me a week-long trip to New Zealand\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb31bf56",
   "metadata": {},
   "source": [
    "The monitoring application is periodical, and is activated in a set time-period, so you need to create a questioning function that is timed, and separates the questioning of the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "66e2b6ba-d864-41d3-b0e2-4dbb81562a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def question_model(questions, serving_function, base_model):\n",
    "    for question in questions:\n",
    "        seconds = random.randint(1, 60)\n",
    "        # Invoking the pretrained model:\n",
    "        serving_function.invoke(\n",
    "            path=f\"/v2/models/{base_model}/infer\",\n",
    "            body={\"inputs\":[question]},\n",
    "        )\n",
    "\n",
    "        time.sleep(seconds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3aaeb589-4460-4273-b030-86430cfd9735",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> 2024-06-20 09:53:38,482 [info] Invoking function: {\"method\":\"POST\",\"path\":\"http://nuclio-model-monitoring-demo-llm-server.default-tenant.svc.cluster.local:8080/v2/models/google-gemma-2b/infer\"}\n",
      "> 2024-06-20 09:54:30,192 [info] Invoking function: {\"method\":\"POST\",\"path\":\"http://nuclio-model-monitoring-demo-llm-server.default-tenant.svc.cluster.local:8080/v2/models/google-gemma-2b/infer\"}\n",
      "> 2024-06-20 09:54:50,117 [info] Invoking function: {\"method\":\"POST\",\"path\":\"http://nuclio-model-monitoring-demo-llm-server.default-tenant.svc.cluster.local:8080/v2/models/google-gemma-2b/infer\"}\n",
      "> 2024-06-20 09:55:46,050 [info] Invoking function: {\"method\":\"POST\",\"path\":\"http://nuclio-model-monitoring-demo-llm-server.default-tenant.svc.cluster.local:8080/v2/models/google-gemma-2b/infer\"}\n",
      "> 2024-06-20 09:56:42,824 [info] Invoking function: {\"method\":\"POST\",\"path\":\"http://nuclio-model-monitoring-demo-llm-server.default-tenant.svc.cluster.local:8080/v2/models/google-gemma-2b/infer\"}\n",
      "> 2024-06-20 09:57:40,796 [info] Invoking function: {\"method\":\"POST\",\"path\":\"http://nuclio-model-monitoring-demo-llm-server.default-tenant.svc.cluster.local:8080/v2/models/google-gemma-2b/infer\"}\n",
      "> 2024-06-20 09:58:06,651 [info] Invoking function: {\"method\":\"POST\",\"path\":\"http://nuclio-model-monitoring-demo-llm-server.default-tenant.svc.cluster.local:8080/v2/models/google-gemma-2b/infer\"}\n",
      "> 2024-06-20 09:58:18,561 [info] Invoking function: {\"method\":\"POST\",\"path\":\"http://nuclio-model-monitoring-demo-llm-server.default-tenant.svc.cluster.local:8080/v2/models/google-gemma-2b/infer\"}\n",
      "> 2024-06-20 09:58:42,443 [info] Invoking function: {\"method\":\"POST\",\"path\":\"http://nuclio-model-monitoring-demo-llm-server.default-tenant.svc.cluster.local:8080/v2/models/google-gemma-2b/infer\"}\n",
      "> 2024-06-20 09:59:42,268 [info] Invoking function: {\"method\":\"POST\",\"path\":\"http://nuclio-model-monitoring-demo-llm-server.default-tenant.svc.cluster.local:8080/v2/models/google-gemma-2b/infer\"}\n",
      "> 2024-06-20 10:00:12,041 [info] Invoking function: {\"method\":\"POST\",\"path\":\"http://nuclio-model-monitoring-demo-llm-server.default-tenant.svc.cluster.local:8080/v2/models/google-gemma-2b/infer\"}\n",
      "> 2024-06-20 10:01:10,945 [info] Invoking function: {\"method\":\"POST\",\"path\":\"http://nuclio-model-monitoring-demo-llm-server.default-tenant.svc.cluster.local:8080/v2/models/google-gemma-2b/infer\"}\n",
      "> 2024-06-20 10:01:18,755 [info] Invoking function: {\"method\":\"POST\",\"path\":\"http://nuclio-model-monitoring-demo-llm-server.default-tenant.svc.cluster.local:8080/v2/models/google-gemma-2b/infer\"}\n",
      "> 2024-06-20 10:01:59,705 [info] Invoking function: {\"method\":\"POST\",\"path\":\"http://nuclio-model-monitoring-demo-llm-server.default-tenant.svc.cluster.local:8080/v2/models/google-gemma-2b/infer\"}\n",
      "> 2024-06-20 10:02:28,589 [info] Invoking function: {\"method\":\"POST\",\"path\":\"http://nuclio-model-monitoring-demo-llm-server.default-tenant.svc.cluster.local:8080/v2/models/google-gemma-2b/infer\"}\n",
      "> 2024-06-20 10:02:57,561 [info] Invoking function: {\"method\":\"POST\",\"path\":\"http://nuclio-model-monitoring-demo-llm-server.default-tenant.svc.cluster.local:8080/v2/models/google-gemma-2b/infer\"}\n"
     ]
    }
   ],
   "source": [
    "question_model(questions=example_questions, serving_function=serving_function, base_model=base_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cbb33a0-971c-42f9-b5ec-afb1212ad1ba",
   "metadata": {},
   "source": [
    "The Grafana model monitoring page shows the base model's scores:</br>\n",
    "<img src=\"../../_static/images/genai-mm-base-grafana-1.png\" width=\"900\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "635f8310-4efb-4ade-a54a-646b5af9b690",
   "metadata": {},
   "source": [
    "As you can see, the base model is not the best at answering a combination of banking and general questions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c064094-0739-4180-93a3-4873f186f995",
   "metadata": {},
   "source": [
    "## Fine-tuning the model with ORPO \n",
    "Now, fine-tune the model using the ORPO algorithm, to align the model to only answer the banking-related questions.\n",
    "\n",
    "[ORPO](https://arxiv.org/abs/2403.07691) is a new method designed to simplify and improve the process of fine-tuning language models to align with user preferences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d105b3a2-77bd-450e-a01c-10e2a424862f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> 2024-06-20 10:31:11,588 [info] Storing function: {\"db\":\"http://mlrun-api:8080\",\"name\":\"train-train\",\"uid\":\"6cfc320db73341b5925610f2a0a77c65\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n",
      "Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n",
      "`config.hidden_activation` if you want to override this behaviour.\n",
      "See https://github.com/huggingface/transformers/pull/29402 for more details.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63968d6ce67e42ab8939b1318ba286d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ae7e269ba254302ab2d3d81089f18b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/728 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf2c6971d6ff46a094b8392244486421",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/8 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98594251136748388db71d60ad092623",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/728 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03968ce09f014ae58df93613e0d468a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/8 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> 2024-06-20 10:31:39,145 [info] training 'mlrun/gemma-2b-bank' based on 'google/gemma-2b'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "Could not estimate the number of tokens of the input, floating-point operations will not be computed\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='91' max='91' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [91/91 17:00, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Runtime</th>\n",
       "      <th>Samples Per Second</th>\n",
       "      <th>Steps Per Second</th>\n",
       "      <th>Rewards/chosen</th>\n",
       "      <th>Rewards/rejected</th>\n",
       "      <th>Rewards/accuracies</th>\n",
       "      <th>Rewards/margins</th>\n",
       "      <th>Logps/rejected</th>\n",
       "      <th>Logps/chosen</th>\n",
       "      <th>Logits/rejected</th>\n",
       "      <th>Logits/chosen</th>\n",
       "      <th>Nll Loss</th>\n",
       "      <th>Log Odds Ratio</th>\n",
       "      <th>Log Odds Chosen</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>1.510200</td>\n",
       "      <td>0.280270</td>\n",
       "      <td>4.985500</td>\n",
       "      <td>1.605000</td>\n",
       "      <td>0.802000</td>\n",
       "      <td>-0.006247</td>\n",
       "      <td>-0.450210</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.443963</td>\n",
       "      <td>-2.251050</td>\n",
       "      <td>-0.031235</td>\n",
       "      <td>-15.340163</td>\n",
       "      <td>-14.761662</td>\n",
       "      <td>0.279368</td>\n",
       "      <td>-0.004511</td>\n",
       "      <td>5.609239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>0.449000</td>\n",
       "      <td>0.182731</td>\n",
       "      <td>5.003500</td>\n",
       "      <td>1.599000</td>\n",
       "      <td>0.799000</td>\n",
       "      <td>-0.000260</td>\n",
       "      <td>-0.322647</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.322387</td>\n",
       "      <td>-1.613233</td>\n",
       "      <td>-0.001300</td>\n",
       "      <td>-13.425655</td>\n",
       "      <td>-13.901117</td>\n",
       "      <td>0.182630</td>\n",
       "      <td>-0.000509</td>\n",
       "      <td>8.189077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57</td>\n",
       "      <td>0.391400</td>\n",
       "      <td>0.191466</td>\n",
       "      <td>4.999700</td>\n",
       "      <td>1.600000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>-0.000050</td>\n",
       "      <td>-0.281829</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.281779</td>\n",
       "      <td>-1.409146</td>\n",
       "      <td>-0.000249</td>\n",
       "      <td>-12.692024</td>\n",
       "      <td>-13.110450</td>\n",
       "      <td>0.191448</td>\n",
       "      <td>-0.000092</td>\n",
       "      <td>9.436110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>76</td>\n",
       "      <td>0.288800</td>\n",
       "      <td>0.167777</td>\n",
       "      <td>5.003500</td>\n",
       "      <td>1.599000</td>\n",
       "      <td>0.799000</td>\n",
       "      <td>-0.000024</td>\n",
       "      <td>-0.260564</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.260539</td>\n",
       "      <td>-1.302818</td>\n",
       "      <td>-0.000121</td>\n",
       "      <td>-13.874061</td>\n",
       "      <td>-14.223096</td>\n",
       "      <td>0.167767</td>\n",
       "      <td>-0.000052</td>\n",
       "      <td>10.020021</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".dictlist {\n",
       "  background-color: #4EC64B;\n",
       "  text-align: center;\n",
       "  margin: 4px;\n",
       "  border-radius: 3px; padding: 0px 3px 1px 3px; display: inline-block;}\n",
       ".artifact {\n",
       "  cursor: pointer;\n",
       "  background-color: #4EC64B;\n",
       "  text-align: left;\n",
       "  margin: 4px; border-radius: 3px; padding: 0px 3px 1px 3px; display: inline-block;\n",
       "}\n",
       "div.block.hidden {\n",
       "  display: none;\n",
       "}\n",
       ".clickable {\n",
       "  cursor: pointer;\n",
       "}\n",
       ".ellipsis {\n",
       "  display: inline-block;\n",
       "  max-width: 60px;\n",
       "  white-space: nowrap;\n",
       "  overflow: hidden;\n",
       "  text-overflow: ellipsis;\n",
       "}\n",
       ".master-wrapper {\n",
       "  display: flex;\n",
       "  flex-flow: row nowrap;\n",
       "  justify-content: flex-start;\n",
       "  align-items: stretch;\n",
       "}\n",
       ".master-tbl {\n",
       "  flex: 3\n",
       "}\n",
       ".master-wrapper > div {\n",
       "  margin: 4px;\n",
       "  padding: 10px;\n",
       "}\n",
       "iframe.fileview {\n",
       "  border: 0 none;\n",
       "  height: 100%;\n",
       "  width: 100%;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       ".pane-header-title {\n",
       "  width: 80%;\n",
       "  font-weight: 500;\n",
       "}\n",
       ".pane-header {\n",
       "  line-height: 1;\n",
       "  background-color: #4EC64B;\n",
       "  padding: 3px;\n",
       "}\n",
       ".pane-header .close {\n",
       "  font-size: 20px;\n",
       "  font-weight: 700;\n",
       "  float: right;\n",
       "  margin-top: -5px;\n",
       "}\n",
       ".master-wrapper .right-pane {\n",
       "  border: 1px inset silver;\n",
       "  width: 40%;\n",
       "  min-height: 300px;\n",
       "  flex: 3\n",
       "  min-width: 500px;\n",
       "}\n",
       ".master-wrapper * {\n",
       "  box-sizing: border-box;\n",
       "}\n",
       "</style><script>\n",
       "function copyToClipboard(fld) {\n",
       "    if (document.queryCommandSupported && document.queryCommandSupported('copy')) {\n",
       "        var textarea = document.createElement('textarea');\n",
       "        textarea.textContent = fld.innerHTML;\n",
       "        textarea.style.position = 'fixed';\n",
       "        document.body.appendChild(textarea);\n",
       "        textarea.select();\n",
       "\n",
       "        try {\n",
       "            return document.execCommand('copy'); // Security exception may be thrown by some browsers.\n",
       "        } catch (ex) {\n",
       "\n",
       "        } finally {\n",
       "            document.body.removeChild(textarea);\n",
       "        }\n",
       "    }\n",
       "}\n",
       "function expandPanel(el) {\n",
       "  const panelName = \"#\" + el.getAttribute('paneName');\n",
       "  console.log(el.title);\n",
       "\n",
       "  document.querySelector(panelName + \"-title\").innerHTML = el.title\n",
       "  iframe = document.querySelector(panelName + \"-body\");\n",
       "\n",
       "  const tblcss = `<style> body { font-family: Arial, Helvetica, sans-serif;}\n",
       "    #csv { margin-bottom: 15px; }\n",
       "    #csv table { border-collapse: collapse;}\n",
       "    #csv table td { padding: 4px 8px; border: 1px solid silver;} </style>`;\n",
       "\n",
       "  function csvToHtmlTable(str) {\n",
       "    return '<div id=\"csv\"><table><tr><td>' +  str.replace(/[\\n\\r]+$/g, '').replace(/[\\n\\r]+/g, '</td></tr><tr><td>')\n",
       "      .replace(/,/g, '</td><td>') + '</td></tr></table></div>';\n",
       "  }\n",
       "\n",
       "  function reqListener () {\n",
       "    if (el.title.endsWith(\".csv\")) {\n",
       "      iframe.setAttribute(\"srcdoc\", tblcss + csvToHtmlTable(this.responseText));\n",
       "    } else {\n",
       "      iframe.setAttribute(\"srcdoc\", this.responseText);\n",
       "    }\n",
       "    console.log(this.responseText);\n",
       "  }\n",
       "\n",
       "  const oReq = new XMLHttpRequest();\n",
       "  oReq.addEventListener(\"load\", reqListener);\n",
       "  oReq.open(\"GET\", el.title);\n",
       "  oReq.send();\n",
       "\n",
       "\n",
       "  //iframe.src = el.title;\n",
       "  const resultPane = document.querySelector(panelName + \"-pane\");\n",
       "  if (resultPane.classList.contains(\"hidden\")) {\n",
       "    resultPane.classList.remove(\"hidden\");\n",
       "  }\n",
       "}\n",
       "function closePanel(el) {\n",
       "  const panelName = \"#\" + el.getAttribute('paneName')\n",
       "  const resultPane = document.querySelector(panelName + \"-pane\");\n",
       "  if (!resultPane.classList.contains(\"hidden\")) {\n",
       "    resultPane.classList.add(\"hidden\");\n",
       "  }\n",
       "}\n",
       "\n",
       "</script>\n",
       "<div class=\"master-wrapper\">\n",
       "  <div class=\"block master-tbl\"><div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>project</th>\n",
       "      <th>uid</th>\n",
       "      <th>iter</th>\n",
       "      <th>start</th>\n",
       "      <th>state</th>\n",
       "      <th>kind</th>\n",
       "      <th>name</th>\n",
       "      <th>labels</th>\n",
       "      <th>inputs</th>\n",
       "      <th>parameters</th>\n",
       "      <th>results</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>model-monitoring-demo</td>\n",
       "      <td><div title=\"6cfc320db73341b5925610f2a0a77c65\"><a href=\"https://dashboard.default-tenant.app.llm-dev.iguazio-cd1.com/mlprojects/model-monitoring-demo/jobs/monitor/6cfc320db73341b5925610f2a0a77c65/overview\" target=\"_blank\" >...a0a77c65</a></div></td>\n",
       "      <td>0</td>\n",
       "      <td>Jun 20 10:31:11</td>\n",
       "      <td>completed</td>\n",
       "      <td>run</td>\n",
       "      <td>train-train</td>\n",
       "      <td><div class=\"dictlist\">v3io_user=zeevr2</div><div class=\"dictlist\">kind=local</div><div class=\"dictlist\">owner=zeevr2</div><div class=\"dictlist\">host=jupyter-gpu-zeev-5b96f58dbb-zpnbq</div></td>\n",
       "      <td></td>\n",
       "      <td><div class=\"dictlist\">dataset=mlrun/banking-orpo</div><div class=\"dictlist\">base_model=google/gemma-2b</div><div class=\"dictlist\">new_model=mlrun/gemma-2b-bank</div><div class=\"dictlist\">device=cuda:0</div></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div></div>\n",
       "  <div id=\"resultd4d753e0-pane\" class=\"right-pane block hidden\">\n",
       "    <div class=\"pane-header\">\n",
       "      <span id=\"resultd4d753e0-title\" class=\"pane-header-title\">Title</span>\n",
       "      <span onclick=\"closePanel(this)\" paneName=\"resultd4d753e0\" class=\"close clickable\">&times;</span>\n",
       "    </div>\n",
       "    <iframe class=\"fileview\" id=\"resultd4d753e0-body\"></iframe>\n",
       "  </div>\n",
       "</div>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<b> > to track results use the .show() or .logs() methods  or <a href=\"https://dashboard.default-tenant.app.llm-dev.iguazio-cd1.com/mlprojects/model-monitoring-demo/jobs/monitor/6cfc320db73341b5925610f2a0a77c65/overview\" target=\"_blank\">click here</a> to open in UI</b>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> 2024-06-20 10:48:51,940 [info] Run execution finished: {\"name\":\"train-train\",\"status\":\"completed\"}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<mlrun.model.RunObject at 0x7fe2278553d0>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "project.run_function(\n",
    "    function=\"train\",\n",
    "    params={\n",
    "        \"dataset\": \"mlrun/banking-orpo\",\n",
    "        \"base_model\": \"google/gemma-2b\",\n",
    "        \"new_model\": \"mlrun/gemma-2b-bank\",\n",
    "        \"device\": \"cuda:0\",\n",
    "    },\n",
    "    handler=\"train\",\n",
    "    outputs=[\"model\"],\n",
    "    # local=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15032f46-9dfc-4d88-87c7-610d26189f9e",
   "metadata": {},
   "source": [
    "## Check the performance of the fine-tuned model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d108d40-f0ce-4536-a046-8a2e39e63133",
   "metadata": {},
   "source": [
    "Now load and deploy the trained model to see how it performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bbac65b0-76ff-419f-910a-6fb856b212f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "serving_function.add_model(\n",
    "    base_model,\n",
    "    class_name=\"LLMModelServer\",\n",
    "    llm_type=\"HuggingFace\",\n",
    "    model_name=\"google/gemma-2b\",\n",
    "    adapter=\"mlrun/gemma-2b-bank-v0.1\", \n",
    "    model_path=f\"store://models/{project.name}/{base_model}:latest\",\n",
    "    generate_kwargs={\n",
    "        \"do_sample\": True,\n",
    "        \"top_p\": 0.9,\n",
    "        \"num_return_sequences\": 1,\n",
    "        \"max_length\": 80,\n",
    "    },\n",
    "    device_map=\"cuda:0\",\n",
    ")\n",
    "serving_function.set_tracking()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e3364b62-8897-4037-832b-51a27490fe75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> 2024-06-20 10:03:24,389 [info] Starting remote function deploy\n",
      "2024-06-20 10:03:24  (info) Deploying function\n",
      "2024-06-20 10:03:24  (info) Building\n",
      "2024-06-20 10:03:25  (info) Staging files and preparing base images\n",
      "2024-06-20 10:03:25  (warn) Using user provided base image, runtime interpreter version is provided by the base image\n",
      "2024-06-20 10:03:25  (info) Building processor image\n",
      "2024-06-20 10:07:55  (info) Build complete\n",
      "2024-06-20 10:08:35  (info) Function deploy complete\n",
      "> 2024-06-20 10:08:36,993 [info] Successfully deployed function: {\"external_invocation_urls\":[\"model-monitoring-demo-llm-server.default-tenant.app.llm-dev.iguazio-cd1.com/\"],\"internal_invocation_urls\":[\"nuclio-model-monitoring-demo-llm-server.default-tenant.svc.cluster.local:8080\"]}\n"
     ]
    }
   ],
   "source": [
    "deployment = serving_function.deploy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ba3cf2a5-d489-4747-9c39-0645be0362d4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> 2024-06-20 10:08:37,041 [info] Invoking function: {\"method\":\"POST\",\"path\":\"http://nuclio-model-monitoring-demo-llm-server.default-tenant.svc.cluster.local:8080/v2/models/google-gemma-2b/infer\"}\n",
      "> 2024-06-20 10:09:09,635 [info] Invoking function: {\"method\":\"POST\",\"path\":\"http://nuclio-model-monitoring-demo-llm-server.default-tenant.svc.cluster.local:8080/v2/models/google-gemma-2b/infer\"}\n",
      "> 2024-06-20 10:09:46,344 [info] Invoking function: {\"method\":\"POST\",\"path\":\"http://nuclio-model-monitoring-demo-llm-server.default-tenant.svc.cluster.local:8080/v2/models/google-gemma-2b/infer\"}\n",
      "> 2024-06-20 10:10:32,056 [info] Invoking function: {\"method\":\"POST\",\"path\":\"http://nuclio-model-monitoring-demo-llm-server.default-tenant.svc.cluster.local:8080/v2/models/google-gemma-2b/infer\"}\n",
      "> 2024-06-20 10:11:12,592 [info] Invoking function: {\"method\":\"POST\",\"path\":\"http://nuclio-model-monitoring-demo-llm-server.default-tenant.svc.cluster.local:8080/v2/models/google-gemma-2b/infer\"}\n",
      "> 2024-06-20 10:11:49,813 [info] Invoking function: {\"method\":\"POST\",\"path\":\"http://nuclio-model-monitoring-demo-llm-server.default-tenant.svc.cluster.local:8080/v2/models/google-gemma-2b/infer\"}\n",
      "> 2024-06-20 10:12:29,504 [info] Invoking function: {\"method\":\"POST\",\"path\":\"http://nuclio-model-monitoring-demo-llm-server.default-tenant.svc.cluster.local:8080/v2/models/google-gemma-2b/infer\"}\n",
      "> 2024-06-20 10:13:00,187 [info] Invoking function: {\"method\":\"POST\",\"path\":\"http://nuclio-model-monitoring-demo-llm-server.default-tenant.svc.cluster.local:8080/v2/models/google-gemma-2b/infer\"}\n",
      "> 2024-06-20 10:13:09,847 [info] Invoking function: {\"method\":\"POST\",\"path\":\"http://nuclio-model-monitoring-demo-llm-server.default-tenant.svc.cluster.local:8080/v2/models/google-gemma-2b/infer\"}\n",
      "> 2024-06-20 10:14:08,553 [info] Invoking function: {\"method\":\"POST\",\"path\":\"http://nuclio-model-monitoring-demo-llm-server.default-tenant.svc.cluster.local:8080/v2/models/google-gemma-2b/infer\"}\n",
      "> 2024-06-20 10:14:44,267 [info] Invoking function: {\"method\":\"POST\",\"path\":\"http://nuclio-model-monitoring-demo-llm-server.default-tenant.svc.cluster.local:8080/v2/models/google-gemma-2b/infer\"}\n",
      "> 2024-06-20 10:15:08,961 [info] Invoking function: {\"method\":\"POST\",\"path\":\"http://nuclio-model-monitoring-demo-llm-server.default-tenant.svc.cluster.local:8080/v2/models/google-gemma-2b/infer\"}\n",
      "> 2024-06-20 10:15:29,635 [info] Invoking function: {\"method\":\"POST\",\"path\":\"http://nuclio-model-monitoring-demo-llm-server.default-tenant.svc.cluster.local:8080/v2/models/google-gemma-2b/infer\"}\n",
      "> 2024-06-20 10:15:32,289 [info] Invoking function: {\"method\":\"POST\",\"path\":\"http://nuclio-model-monitoring-demo-llm-server.default-tenant.svc.cluster.local:8080/v2/models/google-gemma-2b/infer\"}\n",
      "> 2024-06-20 10:16:03,970 [info] Invoking function: {\"method\":\"POST\",\"path\":\"http://nuclio-model-monitoring-demo-llm-server.default-tenant.svc.cluster.local:8080/v2/models/google-gemma-2b/infer\"}\n",
      "> 2024-06-20 10:16:06,620 [info] Invoking function: {\"method\":\"POST\",\"path\":\"http://nuclio-model-monitoring-demo-llm-server.default-tenant.svc.cluster.local:8080/v2/models/google-gemma-2b/infer\"}\n"
     ]
    }
   ],
   "source": [
    "question_model(questions=example_questions, serving_function=serving_function, base_model=base_model) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8815b08b-d9de-4499-994f-9139b104954f",
   "metadata": {},
   "source": [
    "The Grafana model monitoring page shows a high pass rate and a high guardrails score:</br>\n",
    "<img src=\"../../_static/images/genai-mm-base-grafana-2.png\" width=\"900\" >"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How real time pipelines work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A graph is composed of the following:\n",
    "\n",
    "* Step: A Step runs a function or class handler or a REST API call. MLRun comes with a list of [pre-built steps](./available-steps.md) that include data manipulation, readers, writers and model serving. You can also write your own steps using \n",
    "    standard Python functions or custom functions/classes, or can be a external REST API (the special $remote class).\n",
    "* Router: A special type of step is a router with routing logic and multiple child routes/models. The basic \n",
    "    routing logic is to route to the child routes based on the Event.path. More advanced or custom routing can be used,\n",
    "    for example, the ensemble router sends the event to all child routes in parallel, aggregates the result and responds.\n",
    "* Queue: A queue or stream that accepts data from one or more source steps and publishes to one or more output steps. \n",
    "    Queues are best used to connect independent functions/containers. Queues can run in-memory or be implemented using a stream,     which allows it to span processes/containers.\n",
    "    \n",
    "The Graph server has two modes of operation (topologies):\n",
    "\n",
    "* Router topology (default): A minimal configuration with a single router and child tasks/routes. This can be used for simple model serving or single hop configurations.\n",
    "* Flow topology: A full graph/DAG. The flow topology is implemented using two engines: async (the default)\n",
    "is based on [Storey](https://github.com/mlrun/storey) and asynchronous event loop; and `sync`, which supports a simple\n",
    "sequence of steps.\n",
    "\n",
    "The first graph element accepts an `Event` object, transforms/processes the event and passes the result to the next steps \n",
    "in the graph. The final result can be written out to some destination (file, DB, stream, ..) \n",
    "or returned back to the caller (one of the graph steps can be marked with `.respond()`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Event object\n",
    "\n",
    "The Graph state machine accepts an Event object (similar to a Nuclio Event) and passes \n",
    "it along the pipeline. An Event object hosts the event `body` along with other attributes \n",
    "such as `path` (http request path), `method` (GET, POST, ..), and`id` (unique event ID).\n",
    "\n",
    "In some cases the events represent a record with a unique `key`, which can be read/set \n",
    "through the `event.key`. Records have associated `event.time` that, by default, are \n",
    "the arrival time, but can also be set by a step.\n",
    "\n",
    "The Task steps are called with the `event.body` by default. If a task step needs to \n",
    "read or set other event elements (key, path, time, ..) you should set the task `full_event`\n",
    "argument to `True`.\n",
    "\n",
    "Task steps support optional `input_path` and `result_path` attributes that allow controlling which portion of \n",
    "the event is sent as input to the step, and where to update the returned result.\n",
    "\n",
    "For example, for an event body `{\"req\": {\"body\": \"x\"}}`, `input_path=\"req.body\"` and `result_path=\"resp\"` \n",
    "the step gets `\"x\"` as the input. The output after the step is `{\"req\": {\"body\": \"x\"}: \"resp\": <step output>}`.\n",
    "Note that `input_path` and `result_path` do not work together with `full_event=True`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Context object\n",
    "\n",
    "The step classes are initialized with a `context` object (when they have `context` in their `__init__` args).\n",
    "The context is used to pass data and for interfacing with system services. The context object has the \n",
    "following attributes and methods.\n",
    "\n",
    "Attributes:\n",
    "* **logger**: Central logger (Nuclio logger when running in Nuclio).\n",
    "* **verbose**: True if in verbose/debug mode.\n",
    "* **root**: The graph object.\n",
    "* **current_function**: When running in a distributed graph, the current child function name.\n",
    "\n",
    "Methods:\n",
    "* **get_param(key, default=None)**: Get the graph parameter by key. Parameters are set at the\n",
    "  serving function (e.g. `function.spec.parameters = {\"param1\": \"x\"}`).\n",
    "* **get_secret(key)**: Get the value of a project/user secret.\n",
    "* **get_store_resource(uri, use_cache=True)**: Get the mlrun store object (data item, artifact, model, feature set, feature vector).\n",
    "* **get_remote_endpoint(name, external=False)**: Return the remote nuclio/serving function http(s) endpoint given its [project/]function-name[:tag].\n",
    "* **Response(headers=None, body=None, content_type=None, status_code=200)**: Create a nuclio response object, for returning detailed http responses.\n",
    "\n",
    "Example, using the context:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " if self.context.verbose:\n",
    "        self.context.logger.info('my message', some_arg='text')\n",
    "    x = self.context.get_param('x', 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building distributed graphs\n",
    "\n",
    "Graphs can be hosted by a single function (using zero to N containers), or span multiple functions\n",
    "where each function can have its own container image and resources (replicas, GPUs/CPUs, volumes, etc.).\n",
    "It has a `root` function, which is where you configure triggers (http, incoming stream, cron, ..), \n",
    "and optional downstream child functions.\n",
    "\n",
    "You can specify the `function` attribute in `Task` or `Router` steps. This indicates where \n",
    "this step should run. When the `function` attribute is not specified it runs on the root function.</b>\n",
    "`function=\"*\"` means the step can run in any of the child functions.\n",
    "\n",
    "Steps on different functions should be connected using a `Queue` step (a stream)\n",
    "\n",
    "**Adding a child function:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "```python\n",
    "fn.add_child_function('enrich', \n",
    "                      './entity_extraction.ipynb', \n",
    "                      image='mlrun/mlrun',\n",
    "                      requirements=[\"storey\", \"sklearn\"])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See a [complete example](./model-serving-get-started.html#example-nlp-processing-pipeline-with-real-time-streaming).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error handling and catchers\n",
    "\n",
    "Graph steps can raise an exception and you might want to have an error handling flow. ,\n",
    "You can specify an exception handling step/branch that triggers on error.\n",
    "The error handler step receives the event that entered the failed step, with two extra\n",
    "attributes: `event.origin_state` indicates the name of the failed step, and `event.error`\n",
    "holds the error string.\n",
    "\n",
    "Use the `graph.error_handler()` (apply to all steps) or `step.error_handler()` \n",
    "(apply to a specific step) if you want the error from the graph or the step to be \n",
    "fed into a specific step (catcher).\n",
    "\n",
    "Example, setting an error catcher per step: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    graph.add_step(\"MyClass\", name=\"my-class\", after=\"pre-process\").error_handler(\"catcher\")\n",
    "    graph.add_step(\"ErrHandler\", name=\"catcher\", full_event=True, after=\"\")\n",
    "    \n",
    "```{note}\n",
    "Additional steps can follow the `catcher` step.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See the [full example](./use-cases.html#example-advanced-data-processing-and-serving-ensemble)\n",
    "\n",
    "**Exception stream:**\n",
    "\n",
    "The graph errors/exceptions can be pushed into a special error stream. This is very convenient \n",
    "in the case of distributed and production graphs. \n",
    "\n",
    "Set the exception stream address (using v3io streams uri):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    function.spec.error_stream = 'users/admin/my-err-stream'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another example, using a graph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph2.plot(rankdir='LR')\n",
    "\n",
    "<graphviz.dot.Digraph at 0x7fd46e42b810>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have the graph2_enrich which we kept when we built the graph, we can add a error handler in the following way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph2_enrich.error_handler(\"catcher\")\n",
    "graph2.add_step(\"ErrHandler\", name=\"catcher\", full_event=True, after=\"\")\n",
    "\n",
    "out!!!!\n",
    "<mlrun.serving.states.TaskStep at 0x7fd46e557750>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, if we display the graph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph2.plot(rankdir='LR')\n",
    "How to make this out????\n",
    "<graphviz.dot.Digraph at 0x7fd46c36f590>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exception stream\n",
    "\n",
    "The graph errors/exceptions can be pushed into a special error stream, this is very convenient \n",
    "in the case of distributed and production graphs \n",
    "\n",
    "setting the exception stream address (using v3io streams uri):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn_preprocess2.spec.error_stream = err_stream"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

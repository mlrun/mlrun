{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph overview\n",
    "   \n",
    "MLRun Graphs enable building and running DAGs (directed acyclic graph). The first graph element accepts \n",
    "an `Event` object, transforms/processes the event and passes the result to the next steps \n",
    "in the graph. The final result can be written out to some destination (file, DB, stream, ..) \n",
    "or returned back to the caller (one of the graph steps can be marked with `.respond()`).\n",
    "    \n",
    "A graph is composed of the following:\n",
    "    \n",
    "- **Step**: A Step runs a function or class handler or a REST API call. MLRun comes with a list of [pre-built steps](./available-steps.md) that include data manipulation, readers, writers and model serving. You can also [write your own steps](./writing-custom-steps.ipynb) using standard Python functions or custom functions/classes, or can be a external REST API (the special `$remote` class).\n",
    "- **Router**: A special type of step is a router with routing logic and multiple child routes/models. The basic routing logic is to route to the child routes based on the Event.path. More advanced or custom routing can be used, for example, the ensemble router sends the event to all child routes in parallel, aggregates the result and responds.\n",
    "- **Queue**: A queue or stream that accepts data from one or more source steps and publishes to one or more output steps. Queues are best used to connect independent functions/containers. Queues can run in-memory or be implemented using a stream, which allosw it to span processes/containers.\n",
    " \n",
    "The Graph server has two modes of operation (topologies):\n",
    "    \n",
    "- **Router topology** (default): A minimal configuration with a single router and child tasks/routes.\n",
    "This can be used for simple model serving or single hop configurations.\n",
    "- **Flow topology**: A full graph/DAG. The flow topology is implemented using two engines: `async` (the default)\n",
    "      is based on [Storey](https://github.com/mlrun/storey) and asynchronous event loop; and `sync`, which supports a simple\n",
    "      sequence of steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Serving graphs can be composed of pre-defined graph blocks (model servers, routers,\n",
    "ensembles, data readers and writers, data engineering tasks, validators, etc.), \n",
    "or from native python classes/functions. Graphs can auto-scale and span multiple function \n",
    "containers (connected through streaming protocols).\n",
    "\n",
    "![serving graph high level](../_static/images/serving-graph-high-level.png)\n",
    "  \n",
    "Different steps can run on the same local function, or run on a remote function. This allows calling existing functions from the graph and reuse them from other graphs as well as scale up and down different components individually.\n",
    "\n",
    "Graphs can run inside your IDE or Notebook for test and simulation, and can then be deployed \n",
    "into production serverless pipeline with a single command. Serving Graphs are built on \n",
    "top of [Nuclio <https://github.com/nuclio/nuclio](real-time serverless engine), MLRun Jobs, \n",
    "[MLRun Storey <https://github.com/mlrun/storey>](native Python async and stream processing engine), \n",
    "and other MLRun facilities. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
